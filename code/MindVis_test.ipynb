{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb32c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import h5py\n",
    "\n",
    "# Add the mind-vis code folder to path\n",
    "sys.path.append(r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\code\\Mind_Vis_utils\")\n",
    "\n",
    "# mind-vis imports\n",
    "#from sc_mbm.mae_for_fmri import MAEforFMRI, fmri_encoder\n",
    "from Mind_Vis_utils.dc_ldm.ldm_for_fmri import cond_stage_model\n",
    "from Mind_Vis_utils.dc_ldm.ldm_for_fmri import create_model_from_config, fLDM\n",
    "from Mind_Vis_utils.dataset import create_Kamitani_dataset, create_BOLD5000_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d76e0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of imagenet images with descriptions in Bold: 152 out of 1916\n",
      "Number of imagenet images with descriptions in Kamitani: 6 out of 50\n"
     ]
    }
   ],
   "source": [
    "# Imagenet dataset\n",
    "\n",
    "path_mindvis_imagenet_labels = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Image_Labels\\imagenet_final_labels.txt\"\n",
    "path_imagenet_captions = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\ImageNet-captions\\imagenet_captions.json\"\n",
    "path_mindvis_bold_imagenet_files = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\ImageNet\\*\"\n",
    "path_mindvis_kamitani_imagenet_files = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\Kamitani\\npz\\imagenet_testing_label.csv\"\n",
    "\n",
    "with open(path_mindvis_imagenet_labels, 'rb') as f:\n",
    "          mindvis_imagenet_labels = f.read()\n",
    "        \n",
    "\n",
    "mindvis_imagenet_labels = mindvis_imagenet_labels.splitlines()\n",
    "wnid_list = []\n",
    "for line in mindvis_imagenet_labels:\n",
    "    wnid_list.append(line.split()[0])\n",
    "    \n",
    "files_list = []\n",
    "bold_imagenet_list = glob(path_mindvis_bold_imagenet_files)\n",
    "bold_imagenet_list = [os.path.split(img)[1] for img in bold_imagenet_list]\n",
    "\n",
    "kamitani_imagenet_list = pd.read_csv(path_mindvis_kamitani_imagenet_files, sep=',', header=None, names=['a', 'filename'])\n",
    "kamitani_imagenet_list = kamitani_imagenet_list.filename\n",
    "\n",
    "counts = 0\n",
    "with open(path_imagenet_captions) as f:\n",
    "    imnet_cap_json = json.load(f)\n",
    "    for img in imnet_cap_json:\n",
    "        if (img['filename'] in bold_imagenet_list) and (not img['description']):\n",
    "            counts += 1\n",
    "            \n",
    "print(f\"Number of imagenet images with descriptions in Bold: {counts} out of {len(bold_imagenet_list)}\")\n",
    "\n",
    "counts = 0\n",
    "with open(path_imagenet_captions) as f:\n",
    "    imnet_cap_json = json.load(f)\n",
    "    for img in imnet_cap_json:\n",
    "        if (img['filename'] in kamitani_imagenet_list.values) and (not img['description']):\n",
    "            counts += 1\n",
    "            \n",
    "print(f\"Number of imagenet images with descriptions in Kamitani: {counts} out of {len(kamitani_imagenet_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76aa03ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'n01443537_22563.JPEG' in kamitani_imagenet_list.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7707f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO dataset\n",
    "\n",
    "path_coco_captions = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\COCO-captions\\annotations\\captions_train2014.json\"\n",
    "path_coco_images = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\COCO\"\n",
    "\n",
    "with open(path_coco_captions, 'rb') as f:\n",
    "    coco_caption_json = json.load(f)\n",
    "    \n",
    "coco_cap = pd.DataFrame.from_records(coco_caption_json['annotations'])\n",
    "mindvis_coco_list = glob(f\"{path_coco_images}\\*\")\n",
    "mindvis_coco_list = [os.path.split(img)[1] for img in mindvis_coco_list]\n",
    "\n",
    "coco_cap_unique_img_id = coco_cap.image_id.unique()\n",
    "coco_cap_unique_img_id_as_filenames = [f\"COCO_train2014_{img_id:012d}.jpg\" for img_id in coco_cap_unique_img_id]\n",
    "\n",
    "counts = 0\n",
    "for mindvis_coco_img in mindvis_coco_list:\n",
    "    if mindvis_coco_img in coco_cap_unique_img_id_as_filenames:\n",
    "        counts += 1\n",
    "\n",
    "print(f\"Number of Coco images with descriptions: {counts} out of {len(mindvis_coco_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f187a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mindvis_GOD_fmri_encoder = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\fmri_encoder.pth\"\n",
    "path_mindvis_BOLD5000_fmri_encoder = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\BOLD5000\\fmri_encoder.pth\"\n",
    "path_mindvis_GOD_finetuned = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\finetuned.pth\"\n",
    "mindvis_GOD_fmri_encoder_state_dict = torch.load(path_mindvis_GOD_fmri_encoder, map_location='cpu')\n",
    "config = mindvis_GOD_fmri_encoder_state_dict['config']\n",
    "num_voxels = (mindvis_GOD_fmri_encoder_state_dict['model']['pos_embed'].shape[1] - 1)* config.patch_size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mindvis cond_stage_model\n",
    "brain_encoder = fmri_latent_encoder(mindvis_GOD_fmri_encoder_state_dict,num_voxels = 1024, global_pool=False)\n",
    "\n",
    "# mindvis fmri_encoder\n",
    "#brain_encoder = create_model_from_config(config, num_voxels, global_pool=True)\n",
    "#brain_encoder.load_checkpoint(mindvis_GOD_fmri_encoder_state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a4e85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmri_sample = torch.Tensor(dataset_train[0]['fmri'])\n",
    "brain_encoder.forward(fmri_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(mindvis_GOD_fmri_encoder_state_dict.items())[0\n",
    "config_pretrain = mindvis_GOD_fmri_encoder_state_dict['config']\n",
    "\n",
    "num_voxels = (mindvis_GOD_fmri_encoder_state_dict['model']['pos_embed'].shape[1] - 1)* config_pretrain.patch_size\n",
    "brain_encoer_model = MAEforFMRI(num_voxels=num_voxels, patch_size=config_pretrain.patch_size, embed_dim=config_pretrain.embed_dim,\n",
    "                                decoder_embed_dim=config_pretrain.decoder_embed_dim, depth=config_pretrain.depth, \n",
    "                                num_heads=config_pretrain.num_heads, decoder_num_heads=config_pretrain.decoder_num_heads, \n",
    "                                mlp_ratio=config_pretrain.mlp_ratio, focus_range=None, use_nature_img_loss=False) \n",
    "brain_encoer_model.load_state_dict(mindvis_GOD_fmri_encoder_state_dict['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GOD Dataset\n",
    "path_kamitani_GOD = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\Kamitani\\npz\"\n",
    "dataset_train, dataset_test = create_Kamitani_dataset(path_kamitani_GOD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb02d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mindvis fLDM\n",
    "pretrain_mbm_metafile = torch.load(path_mindvis_GOD_finetuned, map_location='cpu')\n",
    "path_gm_pretrain = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\ldm\\label2img\"\n",
    "num_of_voxels = dataset_train.num_voxels\n",
    "generative_model = fLDM(pretrain_mbm_metafile, num_of_voxels, device='cpu', pretrain_root=path_gm_pretrain, global_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8e611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Loading cond_stage_model (fmri_latent_encoder) saved from script gen_eval_get_encoder\n",
    "path_fmri_encoder_from_final_mindvis = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\fmri_encoder_from_final_mindvis.pth\"\n",
    "final_encoder_state_dict = torch.load(path_fmri_encoder_from_final_mindvis, map_location='cpu')\n",
    "saved_encoder = fmri_latent_encoder(final_encoder_state_dict['metafile'], \n",
    "                                    final_encoder_state_dict['num_voxels'], \n",
    "                                    final_encoder_state_dict['cond_dim'],\n",
    "                                    final_encoder_state_dict['global_pool'])\n",
    "\n",
    "print(\"Loading state dict for saved_encoder\")\n",
    "saved_encoder.load_state_dict(final_encoder_state_dict['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e53ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmri_sample = dataset_train.fmri[0]\n",
    "#fmri_embed = saved_encoder.forward(torch.Tensor(hcp['V1']))\n",
    "\n",
    "path_bold_roi = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\BOLD5000_ROI_from_internet\\ROIs\\CSI1\\h5\\CSI1_ROIs_TR3.h5\"\n",
    "with h5py.File(path_bold_roi, 'r') as f:\n",
    "    print(f.keys())\n",
    "    print(len(f['LHLOC']))\n",
    "    print(f['LHPPA'][2].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83491549",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_hcp = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\HCP\\npz\\100206\\HCP_visual_voxel.npz\"\n",
    "path_bold = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\BOLD5000\\BOLD5000_GLMsingle_ROI_betas\\py\\CSI1_GLMbetas-TYPED-FITHRF-GLMDENOISE-RR_allses_LHEarlyVis.npy\"\n",
    "\n",
    "hcp = np.load(path_hcp)\n",
    "bold = np.load(path_bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b006a32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 262 to 291\n",
      "missing keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n",
      "unexpected keys: ['mask_token']\n",
      "Loading state dict for saved_encoder\n"
     ]
    }
   ],
   "source": [
    "#### Working fmri encoder! ####\n",
    "#### Loading cond_stage_model (fmri_latent_encoder) saved from script gen_eval_get_encoder\n",
    "path_fmri_encoder_from_final_mindvis = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\code\\Mind_Vis_utils\\dc_ldm\\pretrains\\fmri_encoder_from_final_mindvis.pth\"\n",
    "final_encoder_state_dict = torch.load(path_fmri_encoder_from_final_mindvis, map_location='cpu')\n",
    "saved_encoder = cond_stage_model(final_encoder_state_dict['metafile'], \n",
    "                                    final_encoder_state_dict['num_voxels'], \n",
    "                                    final_encoder_state_dict['cond_dim'],\n",
    "                                    final_encoder_state_dict['global_pool'])\n",
    "\n",
    "print(\"Loading state dict for saved_encoder\")\n",
    "saved_encoder.load_state_dict(final_encoder_state_dict['model'], strict=False)\n",
    "\n",
    "# Creating GOD Dataset\n",
    "path_kamitani_GOD = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\Kamitani\\npz\"\n",
    "dataset_train, dataset_test = create_Kamitani_dataset(path_kamitani_GOD)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=3)\n",
    "\n",
    "\n",
    "sample = iter(train_dataloader)\n",
    "s1 = sample.next()\n",
    "s1_fmri_batch = s1['fmri'].float()\n",
    "saved_encoder = saved_encoder.float()\n",
    "train_embed = saved_encoder.forward(s1_fmri_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4324dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(s1_fmri_batch.shape)\n",
    "#print(train_embed.shape)\n",
    "p = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\img_dict.npy\"\n",
    "img_dict = np.load(p, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a014fcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(256, 256, 3)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dict['COCO_train2014_000000000036.jpg'].shape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
