{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb32c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import h5py\n",
    "\n",
    "# Add the mind-vis code folder to path\n",
    "sys.path.append(r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\code\\Mind_Vis_utils\")\n",
    "\n",
    "# mind-vis imports\n",
    "#from sc_mbm.mae_for_fmri import MAEforFMRI, fmri_encoder\n",
    "from Mind_Vis_utils.dc_ldm.ldm_for_fmri import cond_stage_model\n",
    "from Mind_Vis_utils.dc_ldm.ldm_for_fmri import create_model_from_config, fLDM\n",
    "from Mind_Vis_utils.dataset import create_Kamitani_dataset, create_BOLD5000_dataset\n",
    "from  dataset import create_BOLD5000_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf379d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  dataset import create_BOLD5000_dataset\n",
    "#import sys\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\fmri_encoder.pth\"\n",
    "#sys.path.append(r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\")\n",
    "train, test = create_BOLD5000_dataset(subjects=['CSI1', 'CSI3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19367db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 262 to 291\n",
      "missing keys: ['mask_token']\n",
      "unexpected keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n"
     ]
    }
   ],
   "source": [
    "# Working fmri_encodor class loaded from pretrain mbm metafile\n",
    "\n",
    "from copy import deepcopy\n",
    "#pretrain_mbm_path = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\fmri_encoder.pth\"\n",
    "#pretrain_mbm_path = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\fmri_encoder_from_final_mindvis.pth\"\n",
    "pretrain_mbm_metafile_path = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrain_metafile.pth\"\n",
    "pretrain_mbm_metafile = torch.load(pretrain_mbm_metafile_path, map_location='cpu')\n",
    "num_voxels = 4656\n",
    "global_pool = False\n",
    "model = create_model_from_config(pretrain_mbm_metafile['metafile']['config'], num_voxels, global_pool)\n",
    "model2 = deepcopy(model)\n",
    "sd = model.load_checkpoint(pretrain_mbm_metafile['metafile']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c76e27b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embed\n"
     ]
    }
   ],
   "source": [
    "#model = model.float()\n",
    "#print(s1['fmri'].shape)\n",
    "#fmri_embed = model.forward(s1['fmri'].float())\n",
    "#print(pretrain_mbm_metafile['metafile']['model'].keys())\n",
    "#print(list(model.named_parameters()))\n",
    "#len(list(model.parameters()))\n",
    "#pretrain_mbm_metafile['metafile']['model']['pos_embed']\n",
    "model_param, _ = list(zip(*list(model.named_parameters())))\n",
    "metafile_param = pretrain_mbm_metafile['metafile']['model'].keys()\n",
    "metafile_param == sd.keys()\n",
    "for p in metafile_param:\n",
    "    if 'embed' in p:\n",
    "        print(p)\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6db6292",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'blocks.0.norm1.weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#torch.all(model.state_dict()['blocks.0.attn.qkv.weight'] == pretrain_mbm_metafile['metafile']['model']['blocks.0.attn.qkv.weight'])\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#l = [(kv[0], torch.all(kv[1]==pretrain_mbm_metafile['metafile']['model'][kv[0]])) for i,kv in enumerate(model.state_dict().items()) if not (i==1 or i==2)]\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m l \u001B[38;5;241m=\u001B[39m [(kv[\u001B[38;5;241m0\u001B[39m], torch\u001B[38;5;241m.\u001B[39mall(kv[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m==\u001B[39msd[kv[\u001B[38;5;241m0\u001B[39m]])) \u001B[38;5;28;01mfor\u001B[39;00m i,kv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(model\u001B[38;5;241m.\u001B[39mstate_dict()\u001B[38;5;241m.\u001B[39mitems()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (i\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m2\u001B[39m)]\n",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#torch.all(model.state_dict()['blocks.0.attn.qkv.weight'] == pretrain_mbm_metafile['metafile']['model']['blocks.0.attn.qkv.weight'])\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#l = [(kv[0], torch.all(kv[1]==pretrain_mbm_metafile['metafile']['model'][kv[0]])) for i,kv in enumerate(model.state_dict().items()) if not (i==1 or i==2)]\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m l \u001B[38;5;241m=\u001B[39m [(kv[\u001B[38;5;241m0\u001B[39m], torch\u001B[38;5;241m.\u001B[39mall(kv[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m==\u001B[39m\u001B[43msd\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkv\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m)) \u001B[38;5;28;01mfor\u001B[39;00m i,kv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(model\u001B[38;5;241m.\u001B[39mstate_dict()\u001B[38;5;241m.\u001B[39mitems()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (i\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m2\u001B[39m)]\n",
      "\u001B[1;31mKeyError\u001B[0m: 'blocks.0.norm1.weight'"
     ]
    }
   ],
   "source": [
    "#torch.all(model.state_dict()['blocks.0.attn.qkv.weight'] == pretrain_mbm_metafile['metafile']['model']['blocks.0.attn.qkv.weight'])\n",
    "#l = [(kv[0], torch.all(kv[1]==pretrain_mbm_metafile['metafile']['model'][kv[0]])) for i,kv in enumerate(model.state_dict().items()) if not (i==1 or i==2)]\n",
    "l = [(kv[0], torch.all(kv[1]==sd[kv[0]])) for i,kv in enumerate(model.state_dict().items()) if not (i==1 or i==2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3be1a7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cls_token', tensor(True)),\n",
       " ('patch_embed.proj.weight', tensor(True)),\n",
       " ('patch_embed.proj.bias', tensor(True)),\n",
       " ('blocks.0.norm1.weight', tensor(False)),\n",
       " ('blocks.0.norm1.bias', tensor(False)),\n",
       " ('blocks.0.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.0.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.0.attn.proj.weight', tensor(True)),\n",
       " ('blocks.0.attn.proj.bias', tensor(True)),\n",
       " ('blocks.0.norm2.weight', tensor(False)),\n",
       " ('blocks.0.norm2.bias', tensor(False)),\n",
       " ('blocks.0.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.0.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.0.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.0.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.1.norm1.weight', tensor(False)),\n",
       " ('blocks.1.norm1.bias', tensor(False)),\n",
       " ('blocks.1.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.1.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.1.attn.proj.weight', tensor(True)),\n",
       " ('blocks.1.attn.proj.bias', tensor(True)),\n",
       " ('blocks.1.norm2.weight', tensor(False)),\n",
       " ('blocks.1.norm2.bias', tensor(False)),\n",
       " ('blocks.1.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.1.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.1.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.1.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.2.norm1.weight', tensor(False)),\n",
       " ('blocks.2.norm1.bias', tensor(False)),\n",
       " ('blocks.2.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.2.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.2.attn.proj.weight', tensor(True)),\n",
       " ('blocks.2.attn.proj.bias', tensor(True)),\n",
       " ('blocks.2.norm2.weight', tensor(False)),\n",
       " ('blocks.2.norm2.bias', tensor(False)),\n",
       " ('blocks.2.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.2.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.2.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.2.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.3.norm1.weight', tensor(False)),\n",
       " ('blocks.3.norm1.bias', tensor(False)),\n",
       " ('blocks.3.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.3.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.3.attn.proj.weight', tensor(True)),\n",
       " ('blocks.3.attn.proj.bias', tensor(True)),\n",
       " ('blocks.3.norm2.weight', tensor(False)),\n",
       " ('blocks.3.norm2.bias', tensor(False)),\n",
       " ('blocks.3.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.3.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.3.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.3.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.4.norm1.weight', tensor(False)),\n",
       " ('blocks.4.norm1.bias', tensor(False)),\n",
       " ('blocks.4.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.4.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.4.attn.proj.weight', tensor(True)),\n",
       " ('blocks.4.attn.proj.bias', tensor(True)),\n",
       " ('blocks.4.norm2.weight', tensor(False)),\n",
       " ('blocks.4.norm2.bias', tensor(False)),\n",
       " ('blocks.4.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.4.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.4.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.4.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.5.norm1.weight', tensor(False)),\n",
       " ('blocks.5.norm1.bias', tensor(False)),\n",
       " ('blocks.5.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.5.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.5.attn.proj.weight', tensor(True)),\n",
       " ('blocks.5.attn.proj.bias', tensor(True)),\n",
       " ('blocks.5.norm2.weight', tensor(False)),\n",
       " ('blocks.5.norm2.bias', tensor(False)),\n",
       " ('blocks.5.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.5.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.5.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.5.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.6.norm1.weight', tensor(False)),\n",
       " ('blocks.6.norm1.bias', tensor(False)),\n",
       " ('blocks.6.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.6.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.6.attn.proj.weight', tensor(True)),\n",
       " ('blocks.6.attn.proj.bias', tensor(True)),\n",
       " ('blocks.6.norm2.weight', tensor(False)),\n",
       " ('blocks.6.norm2.bias', tensor(False)),\n",
       " ('blocks.6.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.6.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.6.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.6.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.7.norm1.weight', tensor(False)),\n",
       " ('blocks.7.norm1.bias', tensor(False)),\n",
       " ('blocks.7.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.7.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.7.attn.proj.weight', tensor(True)),\n",
       " ('blocks.7.attn.proj.bias', tensor(True)),\n",
       " ('blocks.7.norm2.weight', tensor(False)),\n",
       " ('blocks.7.norm2.bias', tensor(False)),\n",
       " ('blocks.7.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.7.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.7.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.7.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.8.norm1.weight', tensor(False)),\n",
       " ('blocks.8.norm1.bias', tensor(False)),\n",
       " ('blocks.8.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.8.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.8.attn.proj.weight', tensor(True)),\n",
       " ('blocks.8.attn.proj.bias', tensor(True)),\n",
       " ('blocks.8.norm2.weight', tensor(False)),\n",
       " ('blocks.8.norm2.bias', tensor(False)),\n",
       " ('blocks.8.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.8.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.8.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.8.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.9.norm1.weight', tensor(False)),\n",
       " ('blocks.9.norm1.bias', tensor(False)),\n",
       " ('blocks.9.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.9.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.9.attn.proj.weight', tensor(True)),\n",
       " ('blocks.9.attn.proj.bias', tensor(True)),\n",
       " ('blocks.9.norm2.weight', tensor(False)),\n",
       " ('blocks.9.norm2.bias', tensor(False)),\n",
       " ('blocks.9.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.9.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.9.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.9.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.10.norm1.weight', tensor(False)),\n",
       " ('blocks.10.norm1.bias', tensor(False)),\n",
       " ('blocks.10.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.10.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.10.attn.proj.weight', tensor(True)),\n",
       " ('blocks.10.attn.proj.bias', tensor(True)),\n",
       " ('blocks.10.norm2.weight', tensor(False)),\n",
       " ('blocks.10.norm2.bias', tensor(False)),\n",
       " ('blocks.10.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.10.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.10.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.10.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.11.norm1.weight', tensor(False)),\n",
       " ('blocks.11.norm1.bias', tensor(False)),\n",
       " ('blocks.11.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.11.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.11.attn.proj.weight', tensor(True)),\n",
       " ('blocks.11.attn.proj.bias', tensor(True)),\n",
       " ('blocks.11.norm2.weight', tensor(False)),\n",
       " ('blocks.11.norm2.bias', tensor(False)),\n",
       " ('blocks.11.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.11.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.11.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.11.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.12.norm1.weight', tensor(False)),\n",
       " ('blocks.12.norm1.bias', tensor(False)),\n",
       " ('blocks.12.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.12.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.12.attn.proj.weight', tensor(True)),\n",
       " ('blocks.12.attn.proj.bias', tensor(True)),\n",
       " ('blocks.12.norm2.weight', tensor(False)),\n",
       " ('blocks.12.norm2.bias', tensor(False)),\n",
       " ('blocks.12.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.12.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.12.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.12.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.13.norm1.weight', tensor(False)),\n",
       " ('blocks.13.norm1.bias', tensor(False)),\n",
       " ('blocks.13.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.13.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.13.attn.proj.weight', tensor(True)),\n",
       " ('blocks.13.attn.proj.bias', tensor(True)),\n",
       " ('blocks.13.norm2.weight', tensor(False)),\n",
       " ('blocks.13.norm2.bias', tensor(False)),\n",
       " ('blocks.13.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.13.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.13.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.13.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.14.norm1.weight', tensor(False)),\n",
       " ('blocks.14.norm1.bias', tensor(False)),\n",
       " ('blocks.14.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.14.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.14.attn.proj.weight', tensor(True)),\n",
       " ('blocks.14.attn.proj.bias', tensor(True)),\n",
       " ('blocks.14.norm2.weight', tensor(False)),\n",
       " ('blocks.14.norm2.bias', tensor(False)),\n",
       " ('blocks.14.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.14.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.14.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.14.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.15.norm1.weight', tensor(False)),\n",
       " ('blocks.15.norm1.bias', tensor(False)),\n",
       " ('blocks.15.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.15.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.15.attn.proj.weight', tensor(True)),\n",
       " ('blocks.15.attn.proj.bias', tensor(True)),\n",
       " ('blocks.15.norm2.weight', tensor(False)),\n",
       " ('blocks.15.norm2.bias', tensor(False)),\n",
       " ('blocks.15.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.15.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.15.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.15.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.16.norm1.weight', tensor(False)),\n",
       " ('blocks.16.norm1.bias', tensor(False)),\n",
       " ('blocks.16.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.16.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.16.attn.proj.weight', tensor(True)),\n",
       " ('blocks.16.attn.proj.bias', tensor(True)),\n",
       " ('blocks.16.norm2.weight', tensor(False)),\n",
       " ('blocks.16.norm2.bias', tensor(False)),\n",
       " ('blocks.16.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.16.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.16.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.16.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.17.norm1.weight', tensor(False)),\n",
       " ('blocks.17.norm1.bias', tensor(False)),\n",
       " ('blocks.17.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.17.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.17.attn.proj.weight', tensor(True)),\n",
       " ('blocks.17.attn.proj.bias', tensor(True)),\n",
       " ('blocks.17.norm2.weight', tensor(False)),\n",
       " ('blocks.17.norm2.bias', tensor(False)),\n",
       " ('blocks.17.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.17.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.17.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.17.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.18.norm1.weight', tensor(False)),\n",
       " ('blocks.18.norm1.bias', tensor(False)),\n",
       " ('blocks.18.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.18.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.18.attn.proj.weight', tensor(True)),\n",
       " ('blocks.18.attn.proj.bias', tensor(True)),\n",
       " ('blocks.18.norm2.weight', tensor(False)),\n",
       " ('blocks.18.norm2.bias', tensor(False)),\n",
       " ('blocks.18.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.18.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.18.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.18.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.19.norm1.weight', tensor(False)),\n",
       " ('blocks.19.norm1.bias', tensor(False)),\n",
       " ('blocks.19.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.19.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.19.attn.proj.weight', tensor(True)),\n",
       " ('blocks.19.attn.proj.bias', tensor(True)),\n",
       " ('blocks.19.norm2.weight', tensor(False)),\n",
       " ('blocks.19.norm2.bias', tensor(False)),\n",
       " ('blocks.19.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.19.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.19.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.19.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.20.norm1.weight', tensor(False)),\n",
       " ('blocks.20.norm1.bias', tensor(False)),\n",
       " ('blocks.20.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.20.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.20.attn.proj.weight', tensor(True)),\n",
       " ('blocks.20.attn.proj.bias', tensor(True)),\n",
       " ('blocks.20.norm2.weight', tensor(False)),\n",
       " ('blocks.20.norm2.bias', tensor(False)),\n",
       " ('blocks.20.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.20.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.20.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.20.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.21.norm1.weight', tensor(False)),\n",
       " ('blocks.21.norm1.bias', tensor(False)),\n",
       " ('blocks.21.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.21.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.21.attn.proj.weight', tensor(True)),\n",
       " ('blocks.21.attn.proj.bias', tensor(True)),\n",
       " ('blocks.21.norm2.weight', tensor(False)),\n",
       " ('blocks.21.norm2.bias', tensor(False)),\n",
       " ('blocks.21.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.21.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.21.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.21.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.22.norm1.weight', tensor(False)),\n",
       " ('blocks.22.norm1.bias', tensor(False)),\n",
       " ('blocks.22.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.22.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.22.attn.proj.weight', tensor(True)),\n",
       " ('blocks.22.attn.proj.bias', tensor(True)),\n",
       " ('blocks.22.norm2.weight', tensor(False)),\n",
       " ('blocks.22.norm2.bias', tensor(False)),\n",
       " ('blocks.22.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.22.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.22.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.22.mlp.fc2.bias', tensor(True)),\n",
       " ('blocks.23.norm1.weight', tensor(False)),\n",
       " ('blocks.23.norm1.bias', tensor(False)),\n",
       " ('blocks.23.attn.qkv.weight', tensor(True)),\n",
       " ('blocks.23.attn.qkv.bias', tensor(True)),\n",
       " ('blocks.23.attn.proj.weight', tensor(True)),\n",
       " ('blocks.23.attn.proj.bias', tensor(True)),\n",
       " ('blocks.23.norm2.weight', tensor(False)),\n",
       " ('blocks.23.norm2.bias', tensor(False)),\n",
       " ('blocks.23.mlp.fc1.weight', tensor(True)),\n",
       " ('blocks.23.mlp.fc1.bias', tensor(True)),\n",
       " ('blocks.23.mlp.fc2.weight', tensor(True)),\n",
       " ('blocks.23.mlp.fc2.bias', tensor(True)),\n",
       " ('norm.weight', tensor(False)),\n",
       " ('norm.bias', tensor(False))]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet dataset\n",
    "\n",
    "path_mindvis_imagenet_labels = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Image_Labels\\imagenet_final_labels.txt\"\n",
    "path_imagenet_captions = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\ImageNet-captions\\imagenet_captions.json\"\n",
    "path_mindvis_bold_imagenet_files = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\ImageNet\\*\"\n",
    "path_mindvis_kamitani_imagenet_files = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\Kamitani\\npz\\imagenet_testing_label.csv\"\n",
    "\n",
    "with open(path_mindvis_imagenet_labels, 'rb') as f:\n",
    "          mindvis_imagenet_labels = f.read()\n",
    "        \n",
    "\n",
    "mindvis_imagenet_labels = mindvis_imagenet_labels.splitlines()\n",
    "wnid_list = []\n",
    "for line in mindvis_imagenet_labels:\n",
    "    wnid_list.append(line.split()[0])\n",
    "    \n",
    "files_list = []\n",
    "bold_imagenet_list = glob(path_mindvis_bold_imagenet_files)\n",
    "bold_imagenet_list = [os.path.split(img)[1] for img in bold_imagenet_list]\n",
    "\n",
    "kamitani_imagenet_list = pd.read_csv(path_mindvis_kamitani_imagenet_files, sep=',', header=None, names=['a', 'filename'])\n",
    "kamitani_imagenet_list = kamitani_imagenet_list.filename\n",
    "\n",
    "counts = 0\n",
    "with open(path_imagenet_captions) as f:\n",
    "    imnet_cap_json = json.load(f)\n",
    "    for img in imnet_cap_json:\n",
    "        if (img['filename'] in bold_imagenet_list) and (not img['description']):\n",
    "            counts += 1\n",
    "            \n",
    "print(f\"Number of imagenet images with descriptions in Bold: {counts} out of {len(bold_imagenet_list)}\")\n",
    "\n",
    "counts = 0\n",
    "with open(path_imagenet_captions) as f:\n",
    "    imnet_cap_json = json.load(f)\n",
    "    for img in imnet_cap_json:\n",
    "        if (img['filename'] in kamitani_imagenet_list.values) and (not img['description']):\n",
    "            counts += 1\n",
    "            \n",
    "print(f\"Number of imagenet images with descriptions in Kamitani: {counts} out of {len(kamitani_imagenet_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'n01443537_22563.JPEG' in kamitani_imagenet_list.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7707f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO dataset\n",
    "\n",
    "path_coco_captions = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\COCO-captions\\annotations\\captions_train2014.json\"\n",
    "path_coco_images = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\COCO\"\n",
    "\n",
    "with open(path_coco_captions, 'rb') as f:\n",
    "    coco_caption_json = json.load(f)\n",
    "    \n",
    "coco_cap = pd.DataFrame.from_records(coco_caption_json['annotations'])\n",
    "mindvis_coco_list = glob(f\"{path_coco_images}\\*\")\n",
    "mindvis_coco_list = [os.path.split(img)[1] for img in mindvis_coco_list]\n",
    "\n",
    "coco_cap_unique_img_id = coco_cap.image_id.unique()\n",
    "coco_cap_unique_img_id_as_filenames = [f\"COCO_train2014_{img_id:012d}.jpg\" for img_id in coco_cap_unique_img_id]\n",
    "\n",
    "counts = 0\n",
    "for mindvis_coco_img in mindvis_coco_list:\n",
    "    if mindvis_coco_img in coco_cap_unique_img_id_as_filenames:\n",
    "        counts += 1\n",
    "\n",
    "print(f\"Number of Coco images with descriptions: {counts} out of {len(mindvis_coco_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f187a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mindvis_GOD_fmri_encoder = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\fmri_encoder.pth\"\n",
    "path_mindvis_BOLD5000_fmri_encoder = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\BOLD5000\\fmri_encoder.pth\"\n",
    "path_mindvis_GOD_finetuned = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\finetuned.pth\"\n",
    "mindvis_GOD_fmri_encoder_state_dict = torch.load(path_mindvis_GOD_fmri_encoder, map_location='cpu')\n",
    "config = mindvis_GOD_fmri_encoder_state_dict['config']\n",
    "num_voxels = (mindvis_GOD_fmri_encoder_state_dict['model']['pos_embed'].shape[1] - 1)* config.patch_size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mindvis cond_stage_model\n",
    "brain_encoder = fmri_latent_encoder(mindvis_GOD_fmri_encoder_state_dict,num_voxels = 1024, global_pool=False)\n",
    "\n",
    "# mindvis fmri_encoder\n",
    "#brain_encoder = create_model_from_config(config, num_voxels, global_pool=True)\n",
    "#brain_encoder.load_checkpoint(mindvis_GOD_fmri_encoder_state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a4e85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmri_sample = torch.Tensor(dataset_train[0]['fmri'])\n",
    "brain_encoder.forward(fmri_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(mindvis_GOD_fmri_encoder_state_dict.items())[0\n",
    "config_pretrain = mindvis_GOD_fmri_encoder_state_dict['config']\n",
    "\n",
    "num_voxels = (mindvis_GOD_fmri_encoder_state_dict['model']['pos_embed'].shape[1] - 1)* config_pretrain.patch_size\n",
    "brain_encoer_model = MAEforFMRI(num_voxels=num_voxels, patch_size=config_pretrain.patch_size, embed_dim=config_pretrain.embed_dim,\n",
    "                                decoder_embed_dim=config_pretrain.decoder_embed_dim, depth=config_pretrain.depth, \n",
    "                                num_heads=config_pretrain.num_heads, decoder_num_heads=config_pretrain.decoder_num_heads, \n",
    "                                mlp_ratio=config_pretrain.mlp_ratio, focus_range=None, use_nature_img_loss=False) \n",
    "brain_encoer_model.load_state_dict(mindvis_GOD_fmri_encoder_state_dict['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GOD Dataset\n",
    "path_kamitani_GOD = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\Kamitani\\npz\"\n",
    "dataset_train, dataset_test = create_Kamitani_dataset(path_kamitani_GOD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb02d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mindvis fLDM\n",
    "pretrain_mbm_metafile = torch.load(path_mindvis_GOD_finetuned, map_location='cpu')\n",
    "path_gm_pretrain = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\ldm\\label2img\"\n",
    "num_of_voxels = dataset_train.num_voxels\n",
    "generative_model = fLDM(pretrain_mbm_metafile, num_of_voxels, device='cpu', pretrain_root=path_gm_pretrain, global_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8e611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Loading cond_stage_model (fmri_latent_encoder) saved from script gen_eval_get_encoder\n",
    "path_fmri_encoder_from_final_mindvis = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\fmri_encoder_from_final_mindvis.pth\"\n",
    "final_encoder_state_dict = torch.load(path_fmri_encoder_from_final_mindvis, map_location='cpu')\n",
    "saved_encoder = fmri_latent_encoder(final_encoder_state_dict['metafile'], \n",
    "                                    final_encoder_state_dict['num_voxels'], \n",
    "                                    final_encoder_state_dict['cond_dim'],\n",
    "                                    final_encoder_state_dict['global_pool'])\n",
    "\n",
    "print(\"Loading state dict for saved_encoder\")\n",
    "saved_encoder.load_state_dict(final_encoder_state_dict['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e53ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmri_sample = dataset_train.fmri[0]\n",
    "#fmri_embed = saved_encoder.forward(torch.Tensor(hcp['V1']))\n",
    "\n",
    "path_bold_roi = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\BOLD5000_ROI_from_internet\\ROIs\\CSI1\\h5\\CSI1_ROIs_TR3.h5\"\n",
    "with h5py.File(path_bold_roi, 'r') as f:\n",
    "    print(f.keys())\n",
    "    print(len(f['LHLOC']))\n",
    "    print(f['LHPPA'][2].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83491549",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_hcp = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\HCP\\npz\\100206\\HCP_visual_voxel.npz\"\n",
    "path_bold = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\BOLD5000\\BOLD5000_GLMsingle_ROI_betas\\py\\CSI1_GLMbetas-TYPED-FITHRF-GLMDENOISE-RR_allses_LHEarlyVis.npy\"\n",
    "\n",
    "hcp = np.load(path_hcp)\n",
    "bold = np.load(path_bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b006a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 262 to 291\n",
      "missing keys: ['mask_token']\n",
      "unexpected keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n",
      "Loading state dict for saved_encoder\n"
     ]
    }
   ],
   "source": [
    "#### Working cond_stage_model (fmri encoder + projection to LDM latent space) ####\n",
    "#### Loading cond_stage_model (fmri_latent_encoder) saved from script gen_eval_get_encoder\n",
    "path_fmri_encoder_from_final_mindvis = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\code\\Mind_Vis_utils\\dc_ldm\\pretrains\\fmri_encoder_from_final_mindvis.pth\"\n",
    "final_encoder_state_dict = torch.load(path_fmri_encoder_from_final_mindvis, map_location='cpu')\n",
    "saved_encoder = cond_stage_model(final_encoder_state_dict['metafile'], \n",
    "                                    final_encoder_state_dict['num_voxels'], \n",
    "                                    final_encoder_state_dict['cond_dim'],\n",
    "                                    final_encoder_state_dict['global_pool'])\n",
    "\n",
    "print(\"Loading state dict for saved_encoder\")\n",
    "saved_encoder.load_state_dict(final_encoder_state_dict['model'], strict=False)\n",
    "\n",
    "# Creating GOD Dataset\n",
    "path_kamitani_GOD = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\Kamitani\\npz\"\n",
    "dataset_train, dataset_test = create_Kamitani_dataset(path_kamitani_GOD)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=3)\n",
    "\n",
    "\n",
    "sample = iter(train_dataloader)\n",
    "s1 = sample.next()\n",
    "s1_fmri_batch = s1['fmri'].float()\n",
    "saved_encoder = saved_encoder.float()\n",
    "train_embed = saved_encoder.forward(s1_fmri_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca782891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_encoder.mae.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678f966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(s1_fmri_batch.shape)\n",
    "#print(train_embed.shape)\n",
    "p = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\img_dict.npy\"\n",
    "img_dict = np.load(p, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a014fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_embed_batch = model2.forward(s1_fmri_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 291, 1024])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_embed_batch.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
