{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb32c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import h5py\n",
    "\n",
    "# Add the mind-vis code folder to path\n",
    "sys.path.append(r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\code\\Mind_Vis_utils\")\n",
    "\n",
    "# mind-vis imports\n",
    "#from sc_mbm.mae_for_fmri import MAEforFMRI, fmri_encoder\n",
    "from Mind_Vis_utils.dc_ldm.ldm_for_fmri import cond_stage_model\n",
    "from Mind_Vis_utils.dc_ldm.ldm_for_fmri import create_model_from_config, fLDM\n",
    "from Mind_Vis_utils.dataset import create_Kamitani_dataset, create_BOLD5000_dataset\n",
    "from  dataset import create_BOLD5000_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf379d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  dataset import create_BOLD5000_dataset\n",
    "#import sys\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\fmri_encoder.pth\"\n",
    "#sys.path.append(r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\")\n",
    "train, test = create_BOLD5000_dataset(subjects=['CSI1', 'CSI3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72c15e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 262 to 291\n",
      "missing keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n",
      "unexpected keys: ['mask_token', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.12.norm1.weight', 'blocks.12.norm1.bias', 'blocks.12.norm2.weight', 'blocks.12.norm2.bias', 'blocks.13.norm1.weight', 'blocks.13.norm1.bias', 'blocks.13.norm2.weight', 'blocks.13.norm2.bias', 'blocks.14.norm1.weight', 'blocks.14.norm1.bias', 'blocks.14.norm2.weight', 'blocks.14.norm2.bias', 'blocks.15.norm1.weight', 'blocks.15.norm1.bias', 'blocks.15.norm2.weight', 'blocks.15.norm2.bias', 'blocks.16.norm1.weight', 'blocks.16.norm1.bias', 'blocks.16.norm2.weight', 'blocks.16.norm2.bias', 'blocks.17.norm1.weight', 'blocks.17.norm1.bias', 'blocks.17.norm2.weight', 'blocks.17.norm2.bias', 'blocks.18.norm1.weight', 'blocks.18.norm1.bias', 'blocks.18.norm2.weight', 'blocks.18.norm2.bias', 'blocks.19.norm1.weight', 'blocks.19.norm1.bias', 'blocks.19.norm2.weight', 'blocks.19.norm2.bias', 'blocks.20.norm1.weight', 'blocks.20.norm1.bias', 'blocks.20.norm2.weight', 'blocks.20.norm2.bias', 'blocks.21.norm1.weight', 'blocks.21.norm1.bias', 'blocks.21.norm2.weight', 'blocks.21.norm2.bias', 'blocks.22.norm1.weight', 'blocks.22.norm1.bias', 'blocks.22.norm2.weight', 'blocks.22.norm2.bias', 'blocks.23.norm1.weight', 'blocks.23.norm1.bias', 'blocks.23.norm2.weight', 'blocks.23.norm2.bias', 'norm.weight', 'norm.bias']\n"
     ]
    }
   ],
   "source": [
    "#pretrain_mbm_path = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\fmri_encoder.pth\"\n",
    "pretrain_mbm_path = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\fmri_encoder_from_final_mindvis.pth\"\n",
    "pretrain_mbm_metafile = torch.load(pretrain_mbm_path, map_location='cpu')\n",
    "num_voxels = 4656\n",
    "global_pool = True\n",
    "model = create_model_from_config(pretrain_mbm_metafile['metafile']['config'], num_voxels, global_pool)\n",
    "model.load_checkpoint(pretrain_mbm_metafile['metafile']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfa4c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4656])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16258752 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(s1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfmri\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m fmri_embed \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfmri\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\code\\Mind_Vis_utils\\sc_mbm\\mae_for_fmri.py:382\u001b[0m, in \u001b[0;36mfmri_encoder.forward\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m imgs\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    381\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(imgs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# N, n_seq, embed_dim\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# N, n_seq, embed_dim\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m latent\n",
      "File \u001b[1;32m~\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\code\\Mind_Vis_utils\\sc_mbm\\mae_for_fmri.py:372\u001b[0m, in \u001b[0;36mfmri_encoder.forward_encoder\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# apply Transformer blocks\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 372\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_pool:\n\u001b[0;32m    374\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mind-cap\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mind-cap\\lib\\site-packages\\timm\\models\\vision_transformer.py:229\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 229\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    230\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mind-cap\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mind-cap\\lib\\site-packages\\timm\\models\\vision_transformer.py:205\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    202\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    203\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)   \u001b[38;5;66;03m# make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m    206\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    207\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16258752 bytes."
     ]
    }
   ],
   "source": [
    "model = model.float()\n",
    "print(s1['fmri'].shape)\n",
    "fmri_embed = model.forward(s1['fmri'].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f903710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'config'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_mbm_metafile['metafile'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet dataset\n",
    "\n",
    "path_mindvis_imagenet_labels = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Image_Labels\\imagenet_final_labels.txt\"\n",
    "path_imagenet_captions = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\ImageNet-captions\\imagenet_captions.json\"\n",
    "path_mindvis_bold_imagenet_files = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\ImageNet\\*\"\n",
    "path_mindvis_kamitani_imagenet_files = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\Kamitani\\npz\\imagenet_testing_label.csv\"\n",
    "\n",
    "with open(path_mindvis_imagenet_labels, 'rb') as f:\n",
    "          mindvis_imagenet_labels = f.read()\n",
    "        \n",
    "\n",
    "mindvis_imagenet_labels = mindvis_imagenet_labels.splitlines()\n",
    "wnid_list = []\n",
    "for line in mindvis_imagenet_labels:\n",
    "    wnid_list.append(line.split()[0])\n",
    "    \n",
    "files_list = []\n",
    "bold_imagenet_list = glob(path_mindvis_bold_imagenet_files)\n",
    "bold_imagenet_list = [os.path.split(img)[1] for img in bold_imagenet_list]\n",
    "\n",
    "kamitani_imagenet_list = pd.read_csv(path_mindvis_kamitani_imagenet_files, sep=',', header=None, names=['a', 'filename'])\n",
    "kamitani_imagenet_list = kamitani_imagenet_list.filename\n",
    "\n",
    "counts = 0\n",
    "with open(path_imagenet_captions) as f:\n",
    "    imnet_cap_json = json.load(f)\n",
    "    for img in imnet_cap_json:\n",
    "        if (img['filename'] in bold_imagenet_list) and (not img['description']):\n",
    "            counts += 1\n",
    "            \n",
    "print(f\"Number of imagenet images with descriptions in Bold: {counts} out of {len(bold_imagenet_list)}\")\n",
    "\n",
    "counts = 0\n",
    "with open(path_imagenet_captions) as f:\n",
    "    imnet_cap_json = json.load(f)\n",
    "    for img in imnet_cap_json:\n",
    "        if (img['filename'] in kamitani_imagenet_list.values) and (not img['description']):\n",
    "            counts += 1\n",
    "            \n",
    "print(f\"Number of imagenet images with descriptions in Kamitani: {counts} out of {len(kamitani_imagenet_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'n01443537_22563.JPEG' in kamitani_imagenet_list.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7707f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO dataset\n",
    "\n",
    "path_coco_captions = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\COCO-captions\\annotations\\captions_train2014.json\"\n",
    "path_coco_images = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\COCO\"\n",
    "\n",
    "with open(path_coco_captions, 'rb') as f:\n",
    "    coco_caption_json = json.load(f)\n",
    "    \n",
    "coco_cap = pd.DataFrame.from_records(coco_caption_json['annotations'])\n",
    "mindvis_coco_list = glob(f\"{path_coco_images}\\*\")\n",
    "mindvis_coco_list = [os.path.split(img)[1] for img in mindvis_coco_list]\n",
    "\n",
    "coco_cap_unique_img_id = coco_cap.image_id.unique()\n",
    "coco_cap_unique_img_id_as_filenames = [f\"COCO_train2014_{img_id:012d}.jpg\" for img_id in coco_cap_unique_img_id]\n",
    "\n",
    "counts = 0\n",
    "for mindvis_coco_img in mindvis_coco_list:\n",
    "    if mindvis_coco_img in coco_cap_unique_img_id_as_filenames:\n",
    "        counts += 1\n",
    "\n",
    "print(f\"Number of Coco images with descriptions: {counts} out of {len(mindvis_coco_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f187a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mindvis_GOD_fmri_encoder = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\fmri_encoder.pth\"\n",
    "path_mindvis_BOLD5000_fmri_encoder = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\BOLD5000\\fmri_encoder.pth\"\n",
    "path_mindvis_GOD_finetuned = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\GOD\\finetuned.pth\"\n",
    "mindvis_GOD_fmri_encoder_state_dict = torch.load(path_mindvis_GOD_fmri_encoder, map_location='cpu')\n",
    "config = mindvis_GOD_fmri_encoder_state_dict['config']\n",
    "num_voxels = (mindvis_GOD_fmri_encoder_state_dict['model']['pos_embed'].shape[1] - 1)* config.patch_size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mindvis cond_stage_model\n",
    "brain_encoder = fmri_latent_encoder(mindvis_GOD_fmri_encoder_state_dict,num_voxels = 1024, global_pool=False)\n",
    "\n",
    "# mindvis fmri_encoder\n",
    "#brain_encoder = create_model_from_config(config, num_voxels, global_pool=True)\n",
    "#brain_encoder.load_checkpoint(mindvis_GOD_fmri_encoder_state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a4e85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmri_sample = torch.Tensor(dataset_train[0]['fmri'])\n",
    "brain_encoder.forward(fmri_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(mindvis_GOD_fmri_encoder_state_dict.items())[0\n",
    "config_pretrain = mindvis_GOD_fmri_encoder_state_dict['config']\n",
    "\n",
    "num_voxels = (mindvis_GOD_fmri_encoder_state_dict['model']['pos_embed'].shape[1] - 1)* config_pretrain.patch_size\n",
    "brain_encoer_model = MAEforFMRI(num_voxels=num_voxels, patch_size=config_pretrain.patch_size, embed_dim=config_pretrain.embed_dim,\n",
    "                                decoder_embed_dim=config_pretrain.decoder_embed_dim, depth=config_pretrain.depth, \n",
    "                                num_heads=config_pretrain.num_heads, decoder_num_heads=config_pretrain.decoder_num_heads, \n",
    "                                mlp_ratio=config_pretrain.mlp_ratio, focus_range=None, use_nature_img_loss=False) \n",
    "brain_encoer_model.load_state_dict(mindvis_GOD_fmri_encoder_state_dict['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GOD Dataset\n",
    "path_kamitani_GOD = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\Kamitani\\npz\"\n",
    "dataset_train, dataset_test = create_Kamitani_dataset(path_kamitani_GOD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb02d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mindvis fLDM\n",
    "pretrain_mbm_metafile = torch.load(path_mindvis_GOD_finetuned, map_location='cpu')\n",
    "path_gm_pretrain = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\ldm\\label2img\"\n",
    "num_of_voxels = dataset_train.num_voxels\n",
    "generative_model = fLDM(pretrain_mbm_metafile, num_of_voxels, device='cpu', pretrain_root=path_gm_pretrain, global_pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8e611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Loading cond_stage_model (fmri_latent_encoder) saved from script gen_eval_get_encoder\n",
    "path_fmri_encoder_from_final_mindvis = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\pretrains\\fmri_encoder_from_final_mindvis.pth\"\n",
    "final_encoder_state_dict = torch.load(path_fmri_encoder_from_final_mindvis, map_location='cpu')\n",
    "saved_encoder = fmri_latent_encoder(final_encoder_state_dict['metafile'], \n",
    "                                    final_encoder_state_dict['num_voxels'], \n",
    "                                    final_encoder_state_dict['cond_dim'],\n",
    "                                    final_encoder_state_dict['global_pool'])\n",
    "\n",
    "print(\"Loading state dict for saved_encoder\")\n",
    "saved_encoder.load_state_dict(final_encoder_state_dict['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e53ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmri_sample = dataset_train.fmri[0]\n",
    "#fmri_embed = saved_encoder.forward(torch.Tensor(hcp['V1']))\n",
    "\n",
    "path_bold_roi = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\BOLD5000_ROI_from_internet\\ROIs\\CSI1\\h5\\CSI1_ROIs_TR3.h5\"\n",
    "with h5py.File(path_bold_roi, 'r') as f:\n",
    "    print(f.keys())\n",
    "    print(len(f['LHLOC']))\n",
    "    print(f['LHPPA'][2].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83491549",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_hcp = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\HCP\\npz\\100206\\HCP_visual_voxel.npz\"\n",
    "path_bold = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Vis-clone\\data\\BOLD5000\\BOLD5000_GLMsingle_ROI_betas\\py\\CSI1_GLMbetas-TYPED-FITHRF-GLMDENOISE-RR_allses_LHEarlyVis.npy\"\n",
    "\n",
    "hcp = np.load(path_hcp)\n",
    "bold = np.load(path_bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b006a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 262 to 291\n",
      "missing keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n",
      "unexpected keys: ['mask_token']\n",
      "Loading state dict for saved_encoder\n"
     ]
    }
   ],
   "source": [
    "#### Working fmri encoder! ####\n",
    "#### Loading cond_stage_model (fmri_latent_encoder) saved from script gen_eval_get_encoder\n",
    "path_fmri_encoder_from_final_mindvis = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\code\\Mind_Vis_utils\\dc_ldm\\pretrains\\fmri_encoder_from_final_mindvis.pth\"\n",
    "final_encoder_state_dict = torch.load(path_fmri_encoder_from_final_mindvis, map_location='cpu')\n",
    "saved_encoder = cond_stage_model(final_encoder_state_dict['metafile'], \n",
    "                                    final_encoder_state_dict['num_voxels'], \n",
    "                                    final_encoder_state_dict['cond_dim'],\n",
    "                                    final_encoder_state_dict['global_pool'])\n",
    "\n",
    "print(\"Loading state dict for saved_encoder\")\n",
    "saved_encoder.load_state_dict(final_encoder_state_dict['model'], strict=False)\n",
    "\n",
    "# Creating GOD Dataset\n",
    "path_kamitani_GOD = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\Kamitani\\npz\"\n",
    "dataset_train, dataset_test = create_Kamitani_dataset(path_kamitani_GOD)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=3)\n",
    "\n",
    "\n",
    "sample = iter(train_dataloader)\n",
    "s1 = sample.next()\n",
    "s1_fmri_batch = s1['fmri'].float()\n",
    "saved_encoder = saved_encoder.float()\n",
    "train_embed = saved_encoder.forward(s1_fmri_batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb7a7b80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "torch.Size([3, 77, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.3553,  0.3056, -0.1592, -0.2413, -0.4026, -0.6836,  0.1891, -0.3148,\n",
       "        -0.0232,  0.2198, -0.0394, -0.0436,  0.1246,  0.2310, -0.3011, -0.0457,\n",
       "         0.1286,  0.2008,  0.2935, -0.0867, -0.4494,  0.0639, -0.2082,  0.2086,\n",
       "         0.5746,  0.1391,  0.1165,  0.1416, -0.2126,  0.3298, -0.3541, -0.1926,\n",
       "         0.2699, -0.3898, -0.4139,  0.2727,  0.0442, -0.3238, -0.4227,  0.2936,\n",
       "        -0.0225, -0.0842,  0.3667, -0.1188, -0.0295, -0.1126,  0.0649,  0.0864,\n",
       "         0.0310, -0.1525, -0.4293,  0.4917,  0.0433,  0.0920, -0.1087, -0.0572,\n",
       "         0.1254,  0.2206,  0.0646, -0.1615,  0.2953, -0.0479, -0.0458,  0.0758,\n",
       "         0.3693, -0.2154,  0.1792, -0.0395, -0.0935,  0.3855,  0.1193, -0.1244,\n",
       "        -0.2060, -0.1670, -0.3660,  0.0216,  0.2244, -0.0947,  0.2395, -0.1049,\n",
       "         0.0121, -0.1868,  0.0284,  0.2794,  0.0233,  0.4056, -0.1268, -0.0527,\n",
       "         0.1688, -0.0165, -0.1059,  0.0485,  0.4269,  0.0499, -0.4801,  0.0738,\n",
       "        -0.2252, -0.1740, -0.0944, -0.3031,  0.5631,  0.2347,  0.0941,  0.5581,\n",
       "        -0.2907, -0.2762,  0.0626, -0.0291, -0.3373, -0.2648, -0.1512, -0.1655,\n",
       "         0.2017,  0.2161, -0.2608,  0.2062, -0.2317, -0.1922, -0.1304, -0.1206,\n",
       "         0.3220, -0.0286,  0.0436, -0.3687,  0.2261,  0.2049, -0.2092, -0.0195,\n",
       "        -0.0783, -0.3224, -0.0532,  0.2449,  0.2178,  0.3693, -0.0915, -0.4498,\n",
       "         0.3896, -0.3206,  0.1379,  0.0010, -0.3783,  0.2007,  0.2152, -0.1552,\n",
       "         0.3262,  0.0710,  0.0016,  0.0195, -0.0435, -0.0731, -0.1688, -0.1401,\n",
       "        -0.0365,  0.3859,  0.1508, -0.3074,  0.0638, -0.2089,  0.1032, -0.0625,\n",
       "         0.0556, -0.0513, -0.1141, -0.3677,  0.0317,  0.3217, -0.0820, -0.1964,\n",
       "         0.0847, -0.1160, -0.1226, -0.0154,  0.2744, -0.1915, -0.2730, -0.4290,\n",
       "         0.0010, -0.1876,  0.2343, -0.0338, -0.6092, -0.0944, -0.0301, -0.0208,\n",
       "         0.5617, -0.1780,  0.3853,  0.1971,  0.3601,  0.4829, -0.1566,  0.1598,\n",
       "        -0.2360, -0.3778, -0.1166, -0.2586,  0.2019, -0.0537,  0.1587, -0.3325,\n",
       "        -0.0845,  0.1780, -0.1214,  0.1158, -0.0023, -0.0242, -0.4191, -0.1017,\n",
       "         0.2505, -0.4460,  0.1189, -0.1293, -0.1744, -0.0309,  0.0491, -0.0106,\n",
       "        -0.6519,  0.2217,  0.0879,  0.0515,  0.1189,  0.4012,  0.2224, -0.5796,\n",
       "         0.1088,  0.2993, -0.1318, -0.2566, -0.0262,  0.2353, -0.0136,  0.2848,\n",
       "        -0.4226, -0.1259,  0.2062, -0.2765,  0.1110, -0.0800,  0.0752, -0.1576,\n",
       "         0.1431,  0.4002,  0.2887,  0.1798, -0.1773, -0.5004, -0.1188, -0.0431,\n",
       "        -0.1886,  0.1148, -0.1186,  0.1127, -0.2600, -0.2143,  0.0687,  0.1291,\n",
       "         0.1862,  0.3031, -0.1605, -0.0413,  0.2265, -0.0166,  0.4115, -0.0465,\n",
       "         0.3733, -0.0070, -0.0116, -0.1534,  0.1328,  0.0285,  0.1407, -0.3043,\n",
       "         0.1094,  0.3852, -0.4868, -0.1431,  0.1622,  0.1307, -0.3837, -0.2109,\n",
       "        -0.2348,  0.1758, -0.3439,  0.0181, -0.5720, -0.2451, -0.2856, -0.1043,\n",
       "         0.1185, -0.3939,  0.2994, -0.1944, -0.1973, -0.1850, -0.1044, -0.0212,\n",
       "         0.2189, -0.0117,  0.1650, -0.1526,  0.0386,  0.2808,  0.1661,  0.5349,\n",
       "        -0.1647,  0.1993,  0.1879, -0.3337, -0.0714,  0.6425, -0.2600,  0.2462,\n",
       "        -0.1786, -0.1307, -0.1439, -0.2896, -0.1338, -0.1098,  0.4659,  0.2714,\n",
       "        -0.4171,  0.0169,  0.0475, -0.1331,  0.0238, -0.3419,  0.0629,  0.3066,\n",
       "         0.4304,  0.0617, -0.0939, -0.1963,  0.1907,  0.1620,  0.4702, -0.2096,\n",
       "         0.0552,  0.4298,  0.1951, -0.1252,  0.2885, -0.0934,  0.1616,  0.4607,\n",
       "        -0.1443,  0.1240,  0.3703, -0.0016, -0.2478, -0.3096,  0.0183,  0.0377,\n",
       "         0.0416,  0.2808, -0.2428, -0.0858,  0.3124,  0.0606,  0.1358, -0.0942,\n",
       "         0.1635,  0.2067, -0.2115,  0.4955, -0.2213, -0.1544,  0.3010,  0.0738,\n",
       "         0.4260,  0.0033, -0.1316, -0.4725, -0.0923, -0.3228,  0.1804, -0.5392,\n",
       "         0.0169, -0.2117, -0.2000, -0.1636,  0.4122, -0.2401,  0.0481,  0.3468,\n",
       "         0.3133, -0.4253, -0.0511,  0.5899, -0.1751,  0.0752, -0.0635, -0.8037,\n",
       "         0.0854, -0.2358,  0.1701,  0.2627,  0.0718, -0.2843,  0.0849,  0.3469,\n",
       "        -0.2266, -0.0816,  0.0201, -0.1584, -0.0848, -0.0823, -0.3544,  0.1596,\n",
       "        -0.1404,  0.0372,  0.2295, -0.1076, -0.0481,  0.1658,  0.0665, -0.2011,\n",
       "        -0.1544, -0.4180, -0.1239, -0.3164,  0.1234, -0.2023, -0.0905,  0.1018,\n",
       "         0.3340,  0.0670, -0.0770, -0.2347, -0.2283,  0.1088, -0.1006, -0.1747,\n",
       "        -0.3113,  0.0237,  0.0325, -0.0157,  0.2613,  0.0198,  0.0113, -0.0105,\n",
       "        -0.1504, -0.2479, -0.0614,  0.4162,  0.1927,  0.0184,  0.2215, -0.0053,\n",
       "        -0.1183, -0.3669, -0.0589,  0.1228, -0.2118,  0.2484,  0.3782,  0.2769,\n",
       "         0.0463,  0.1462,  0.0391, -0.2568,  0.1837, -0.0502, -0.0212,  0.1928,\n",
       "        -0.3117,  0.1727,  0.0307,  0.3850, -0.0704,  0.2450,  0.2099, -0.1175,\n",
       "        -0.0868,  0.0075, -0.0245,  0.2298,  0.0760,  0.1944,  0.4329, -0.2594,\n",
       "        -0.0672,  0.3653,  0.1082, -0.3023,  0.0936,  0.0566, -0.0885, -0.1120,\n",
       "        -0.1120, -0.3163, -0.0995,  0.1212,  0.1104, -0.3013, -0.3326, -0.1240,\n",
       "         0.1142, -0.1133,  0.2771, -0.1735, -0.1572,  0.1147,  0.0072, -0.0325,\n",
       "        -0.1451,  0.0821, -0.2308,  0.1003,  0.4279,  0.3342, -0.0442, -0.0721],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final_encoder_state_dict['cond_dim'])\n",
    "#dataset_train.fmri[0].shape\n",
    "print(train_embed.shape)\n",
    "train_embed[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(s1_fmri_batch.shape)\n",
    "#print(train_embed.shape)\n",
    "p = r\"C:\\Users\\roeys\\OneDrive - Technion\\Semester 7\\DL\\Project\\Mind-Cap\\Mind-Cap\\data\\BOLD5000\\BOLD5000_Stimuli\\Scene_Stimuli\\Presented_Stimuli\\img_dict.npy\"\n",
    "img_dict = np.load(p, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dict['COCO_train2014_000000000036.jpg'].shape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
