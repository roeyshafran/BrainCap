{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home1/roeyshafran/BrainCap/Mind-Cap/code/Mind_Vis_utils/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfmri_caption\u001b[39;00m \u001b[39mimport\u001b[39;00m GPTCaptionModel, create_fmri_encoder_from_pretrained,top_k_top_p_filtering, set_parameter_requires_grad\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m BOLD5000_dataset, identity\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m create_BOLD5000_dataset\n",
      "File \u001b[0;32m/home1/roeyshafran/BrainCap/Mind-Cap/code/fmri_caption.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home1/roeyshafran/BrainCap/Mind-Cap/code/Mind_Vis_utils/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'/home1/roeyshafran/BrainCap/Mind-Cap/code/Mind_Vis_utils/')\n",
    "\n",
    "from fmri_caption import GPTCaptionModel, create_fmri_encoder_from_pretrained,top_k_top_p_filtering, set_parameter_requires_grad\n",
    "from dataset import BOLD5000_dataset, identity\n",
    "from dataset import create_BOLD5000_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from datetime import datetime\n",
    "#import optuna\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 8\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "LEARNING_RATE = 1.25e-5\n",
    "NUM_EPOCHS = 16\n",
    "TRIAL_NUM_TRAIN_EXAMPLES = BATCH_SIZE*30\n",
    "TRIAL_NUM_VAL_EXAMPLES = BATCH_SIZE*10\n",
    "WARMUP_STEPS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pretrained files\n",
    "path_fmri_encoder = r\"/databases/roeyshafran/BrainCap/pretrains/pretrain_metafile.pth\"\n",
    "#path_BOLD_dataset = r\"/databases/roeyshafran/BrainCap/data/BOLD5000/CSI1_dataset.pth\"\n",
    "path_BOLD_dataset = r\"/databases/roeyshafran/BrainCap/data/CSI1_no_duplicates.pth\"\n",
    "\n",
    "# create BOLD5000 dataset\n",
    "BOLD_dataset = torch.load(path_BOLD_dataset)\n",
    "bold_train, bold_test = BOLD_dataset['train'], BOLD_dataset['test']\n",
    "#num_voxels = bold_test.num_voxels\n",
    "num_voxels = 1696\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original train len: {len(bold_train)}, Original test len {len(bold_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_idx, val_idx = train_test_split(list(range(len(bold_train))),test_size=0.1)\n",
    "bold_val = Subset(bold_train, val_idx)\n",
    "bold_train = Subset(bold_train, train_idx)\n",
    "\n",
    "train_dl = DataLoader(bold_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(bold_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(bold_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Train len: {len(bold_train)}, val len: {len(bold_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_duplicates_from_dataset(dataloader):\n",
    "    prev_seen = {\n",
    "            'caption': np.array([]),\n",
    "            'image': torch.tensor([]).to(device),\n",
    "            'fmri': torch.tensor([]).to(device)\n",
    "        }\n",
    "    for batch in dataloader:\n",
    "        #batch['fmri'] = batch['fmri'].to(device)\n",
    "        #batch['image'] = batch['image'].to(device)\n",
    "        remove_duplicates_in_batch(batch) # in-place\n",
    "        remove_previously_seen_fmri(batch, prev_seen, device) # in-place\n",
    "        \n",
    "        try:\n",
    "            prev_seen['caption'] = np.concatenate((prev_seen['caption'], batch['caption']))\n",
    "        except:\n",
    "            prev_seen['caption'] = np.concatenate((prev_seen['caption'], [batch['caption']]))\n",
    "        prev_seen['image'] = torch.cat((prev_seen['image'], batch['image'].to(device)), dim=0)\n",
    "        prev_seen['fmri'] = torch.cat((prev_seen['fmri'], batch['fmri'].to(device)), dim=0)\n",
    "\n",
    "    #return [dict(zip(prev_seen,t)) for t in zip(*prev_seen.values())]\n",
    "    return BOLD5000_dataset(prev_seen['fmri'].cpu(), prev_seen['caption'].cpu(), prev_seen['image'].cpu(), identity, identity, num_voxels)\n",
    "\n",
    "print(f\"With duplicates: Train len: {len(bold_train)}, Test len: {len(bold_test)}\")\n",
    "bold_train = remove_duplicates_from_dataset(train_dl)\n",
    "bold_test = remove_duplicates_from_dataset(test_dl)\n",
    "print(f\"No duplicates: Train len: {len(bold_train)}, Test len: {len(bold_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_voxels = 1696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "to_save = {'train': bold_train, 'test': bold_test}\n",
    "torch.save(to_save, r\"/databases/roeyshafran/BrainCap/data/CSI1_no_duplicates.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Function Delerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_semantic_similarity(generated_caption, real_caption, device):\n",
    "    sentence_model = SentenceTransformer('all-mpnet-base-v2').to(device)\n",
    "    embed_generated = sentence_model.encode(generated_caption, convert_to_tensor=True)\n",
    "    embed_real_caption = sentence_model.encode(real_caption, convert_to_tensor=True)\n",
    "      \n",
    "    return torch.diagonal(util.pytorch_cos_sim(embed_generated, embed_real_caption))\n",
    "\n",
    "def define_GPTCaption_model(encoder, trial=None, projection_sizes=None, use_dropout=False):\n",
    "    if trial:\n",
    "        # TODO: Add Optuna support. If trial is used, override projection_sizes with Optuna suggestion\n",
    "        num_layers = trial.suggest_int(\"num_projection_layers\", 1, 5)\n",
    "        projection_sizes = [encoder.embed_dim]*num_layers\n",
    "    gpt_decoder = GPTCaptionModel(encoder.num_patches, encoder.embed_dim, projection_sizes, use_dropout=use_dropout)\n",
    "    set_parameter_requires_grad(gpt_decoder.embedding_space_projection, feature_extraction=False)\n",
    "    set_parameter_requires_grad(gpt_decoder.gpt, feature_extraction=True)\n",
    "    #set_parameter_requires_grad(gpt_decoder.tokenizer, feature_extraction=True)\n",
    "    #for param in gpt_decoder.tokenizer.parameters():\n",
    "    #  param.requires_grad = False\n",
    "\n",
    "    return gpt_decoder\n",
    "\n",
    "def print_batch(batch, fontsize=5, num_of_columns=5, caption_as_title=False):\n",
    "    N = int(np.ceil(np.sqrt(len(batch))))\n",
    "    num_of_plots = len(batch)\n",
    "    num_of_rows = num_of_plots // num_of_columns\n",
    "    fig, axs = plt.subplots(num_of_rows, num_of_columns)\n",
    "    if hasattr(axs, '__iter__'):\n",
    "        axs = axs.flatten()\n",
    "    else:\n",
    "        axs = [axs]\n",
    "    for idx, ax in enumerate(axs):\n",
    "        try:\n",
    "            ax.imshow(batch[idx]['image'])\n",
    "            str_to_show = f\"({idx}) Generated: {batch[idx]['caption']}\\n   Real: {batch[idx]['real_caption']}  Accuracy: {batch[idx]['accuracy']}\"\n",
    "            if caption_as_title:\n",
    "                ax.set_title(str_to_show, fontsize=fontsize)\n",
    "            else:\n",
    "                print(str_to_show)\n",
    "        except IndexError:\n",
    "            pass\n",
    "        ax.set_xticks([],[])\n",
    "        ax.set_yticks([],[])\n",
    "      \n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_accuracy_on_test(encoder, decoder, dataloader, device, threshhold=0.5, return_best_batch=False):\n",
    "    running_accuracy = 0\n",
    "    above_threshhold_count = 0\n",
    "    best_accuracy = 0\n",
    "    for batch in dataloader:\n",
    "        fmri_prefix = encoder.forward(batch['fmri'].to(device))\n",
    "        generated_caption = decoder.generate_caption(fmri_prefix, device)\n",
    "\n",
    "        accuracy_tensor = calculate_semantic_similarity(generated_caption, batch['caption'], device)\n",
    "        above_threshhold_count += torch.numel(accuracy_tensor[accuracy_tensor >= threshhold])\n",
    "        accuracy = torch.mean(accuracy_tensor).item()\n",
    "        running_accuracy += accuracy\n",
    "\n",
    "        if return_best_batch and (accuracy > best_accuracy):\n",
    "            best_accuracy = accuracy\n",
    "            fields = ['accuracy', 'caption', 'real_caption', 'image']\n",
    "            best_batch = [dict(zip(fields, t)) for t in zip(accuracy_tensor, generated_caption, batch['caption'], batch['image'])]\n",
    "            #best_batch = list(zip(generated_caption, batch['caption'], batch['image']))\n",
    "\n",
    "    accuracy = (running_accuracy / len(dataloader), above_threshhold_count / (len(dataloader)*dataloader.batch_size)) \n",
    "    if return_best_batch:\n",
    "        return accuracy, best_batch\n",
    "    else:\n",
    "        return accuracy\n",
    "        \n",
    "def get_k_best(encoder, decoder, dataloader, k, device):\n",
    "    k_best = []\n",
    "    for batch in dataloader:\n",
    "        remove_duplicates_in_batch(batch) # in-place\n",
    "        remove_previously_seen_fmri(batch, k_best, device) # in-place\n",
    "        fmri_prefix = encoder.forward(batch['fmri'].to(device))\n",
    "        generated_caption = decoder.generate_caption(fmri_prefix, device)\n",
    "        acc = calculate_semantic_similarity(generated_caption, batch['caption'], device).tolist()\n",
    "        fields = ['accuracy', 'caption', 'real_caption', 'image', 'fmri']\n",
    "        d = [dict(zip(fields, t)) for t in zip(acc, generated_caption, batch['caption'], batch['image'], batch['fmri'])] # list of {'accuracy':accuracy, 'generated':generated caption, 'real': labeled caption}\n",
    "        d.extend(k_best)\n",
    "        k_best = heapq.nlargest(k, d, key=lambda s:s['accuracy'])\n",
    "\n",
    "    return k_best\n",
    "\n",
    "def get_k_best_torch(encoder, decoder, dataloader, k, device):\n",
    "    k_best = {\n",
    "        'accuracy': torch.tensor([]).to(device),\n",
    "        'caption': np.array([]),\n",
    "        'real_caption': np.array([]),\n",
    "        'image': torch.tensor([]).to(device),\n",
    "        'fmri': torch.tensor([]).to(device)\n",
    "    }\n",
    "    for batch in dataloader:\n",
    "        #batch['fmri'] = batch['fmri'].to(device)\n",
    "        #batch['image'] = batch['image'].to(device)\n",
    "        remove_duplicates_in_batch(batch) # in-place\n",
    "        remove_previously_seen_fmri(batch, k_best, device) # in-place\n",
    "        fmri_prefix = encoder.forward(batch['fmri'].to(device))\n",
    "        generated_caption = decoder.generate_caption(fmri_prefix, device)\n",
    "        acc = calculate_semantic_similarity(generated_caption, batch['caption'], device)\n",
    "\n",
    "        k_best['accuracy'] = torch.cat((k_best['accuracy'], acc), dim=0)\n",
    "        k_best['caption'] = np.concatenate((k_best['caption'], generated_caption))\n",
    "        k_best['real_caption'] = np.concatenate((k_best['real_caption'], batch['caption']))\n",
    "        k_best['image'] = torch.cat((k_best['image'], batch['image'].to(device)), dim=0)\n",
    "        k_best['fmri'] = torch.cat((k_best['fmri'], batch['fmri'].to(device)), dim=0)\n",
    "\n",
    "        \n",
    "        k_to_use = min(k_best['accuracy'].size(dim=0), k) \n",
    "        topk_values, topk_indices = torch.topk(k_best['accuracy'], k=k_to_use, dim=0) \n",
    "\n",
    "\n",
    "        k_best['accuracy'] = k_best['accuracy'][topk_indices]\n",
    "        k_best['image'] = k_best['image'][topk_indices]\n",
    "        k_best['fmri'] = k_best['fmri'][topk_indices]\n",
    "        k_best['caption'] = k_best['caption'][topk_indices.cpu()]\n",
    "        k_best['real_caption'] = k_best['real_caption'][topk_indices.cpu()]\n",
    "\n",
    "\n",
    "    return k_best\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def remove_previously_seen_fmri(batch, k_best, device):\n",
    "    # Works in-line\n",
    "    #k_best_fmri = torch.tensor([]) if not k_best else torch.cat([torch.unsqueeze(s['fmri'], dim=0) for s in k_best], dim=0)\n",
    "    #print(batch['fmri'].device)\n",
    "    #print(k_best['fmri'].device)\n",
    "    duplicated_mask = torch.isin(batch['fmri'].to(device), k_best['fmri'])[:, 0, 0]\n",
    "    duplicated_mask = ~duplicated_mask\n",
    "    batch['fmri'] = batch['fmri'][duplicated_mask]\n",
    "    batch['image'] = batch['image'][duplicated_mask]\n",
    "    batch['caption'] = batch['caption'][duplicated_mask.cpu()]\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def remove_duplicates_in_batch(batch):\n",
    "    # Works in-place\n",
    "    unique_fmri, idx = unique(batch['fmri'], dim=0)\n",
    "    batch['fmri'] = unique_fmri\n",
    "    batch['image']  = torch.index_select(batch['image'], dim=0, index=idx)\n",
    "    batch['caption'] = np.array(batch['caption'])[idx.cpu()]\n",
    "\n",
    "    return\n",
    "    \n",
    "\n",
    "def unique(x, dim=-1):\n",
    "    unique, inverse = torch.unique(x, return_inverse=True, dim=dim)\n",
    "    perm = torch.arange(inverse.size(dim), dtype=inverse.dtype, device=inverse.device)\n",
    "    inverse, perm = inverse.flip([dim]), perm.flip([dim])\n",
    "    return unique, inverse.new_empty(unique.size(dim)).scatter_(dim, inverse, perm)\n",
    "\n",
    "def objective(encoder, decoder, train_dl, val_dl, device, trial=None):\n",
    "\n",
    "    # Generate the optimizers\n",
    "    if trial:\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "        optimizer_name = trial.suggest_categorial(\"optimizer\", ['Adam', 'AdamW', \"SGD\"])\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr)\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 1, 10)\n",
    "    else:\n",
    "        lr = LEARNING_RATE\n",
    "        optimizer = optim.AdamW(encoder.parameters(), lr)\n",
    "        batch_size = BATCH_SIZE\n",
    "        #scheduler = get_linear_schedule_with_warmup(\n",
    "    #optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=NUM_EPOCHS*len(train_dl)\n",
    "    #)\n",
    "\n",
    "    decoder.train()\n",
    "    encoder.eval()\n",
    "    print(\"\\n\\n\")\n",
    "    running_loss = []\n",
    "    running_semantic_accuracy = []\n",
    "    val_accuracy = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"** Starting epoch {epoch} **\")\n",
    "        with tqdm(train_dl, unit='batch') as tepoch:\n",
    "            semantic_accuracy = 0\n",
    "            for batch_idx, batch in enumerate(tepoch):\n",
    "                if batch_idx * batch_size >= TRIAL_NUM_TRAIN_EXAMPLES:\n",
    "                    break\n",
    "                \n",
    "                tepoch.set_description(f\"Epoch: {epoch}\")\n",
    "\n",
    "                #batch_fmri = batch['fmri'].to(device)\n",
    "                batch_fmri = batch['fmri']\n",
    "                batch_fmri = batch_fmri.to(device)\n",
    "                batch_caption = batch['caption']\n",
    "\n",
    "                #print(f\">>>> encoding fmri scans \", end=\"\")\n",
    "                fmri_prefix = encoder.forward(batch_fmri)\n",
    "                #print(f\"-> tokenizing captions \", end=\"\")\n",
    "                tokens, attention_mask = decoder.tokenizer(batch_caption, return_tensors=\"pt\", padding=True).values()\n",
    "                tokens, attention_mask, fmri_prefix = tokens.to(device), attention_mask.to(device), fmri_prefix.to(device)\n",
    "                #print(f\"-> decoding \")\n",
    "                outputs = decoder.forward(tokens, fmri_prefix, attention_mask)\n",
    "                logits = outputs.logits[:, decoder.prefix_length-1:-1]\n",
    "\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.reshape(-1, logits.shape[-1]),\n",
    "                    tokens.flatten(),\n",
    "                    ignore_index=decoder.tokenizer.pad_token_id\n",
    "                      )\n",
    "                decoder.zero_grad(set_to_none=True)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #scheduler.step()\n",
    "                #print(f\">>>> batch {batch_idx} finished\", end=\"\\r\")\n",
    "\n",
    "                # Eval semantic accuracy\n",
    "                if batch_idx % 10 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        generated_caption = decoder.generate_caption(fmri_prefix, device)\n",
    "                        semantic_accuracy = torch.mean(calculate_semantic_similarity(generated_caption, batch_caption, device)).item()\n",
    "                        running_semantic_accuracy.append(semantic_accuracy)\n",
    "                        running_loss.append(loss.item())\n",
    "                \n",
    "                tepoch.set_postfix(loss=loss.item(), train_accuracy=semantic_accuracy)\n",
    "\n",
    "                # Free GPU memory\n",
    "                del batch_fmri, batch_caption, tokens, attention_mask, logits, outputs, loss\n",
    "                torch.cuda.empty_cache()\n",
    "        val_accuracy.append(calculate_accuracy_on_test(encoder, decoder, val_dl, device, return_best_batch=False))\n",
    "        print(f\"---- epoch loss: {np.mean(running_loss[int(np.floor((epoch*len(train_dl))/10 + 1)):-1]):.4}, test accuracy: {np.mean(running_semantic_accuracy[int(np.floor((epoch*len(train_dl))/10 + 1)):-1]):.4}, validation accuracy: {val_accuracy[epoch]:.4} ---- \")\n",
    "\n",
    "        trial.report(val_accuracy, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_accuracy\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(study_name=\"BOLD5000-caption\", direction=\"maximize\", sampler=sampler)\n",
    "study.optimie(objective, n_trials=100, timeout=1000)\n",
    "\n",
    "pruned_trails = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finished trials: \", len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\" Value: \", trial.value)\n",
    "\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importance(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study, params=['lr', 'batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(decoder.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get encoder-decoder\n",
    "encoder = create_fmri_encoder_from_pretrained(path_fmri_encoder, num_voxels, feature_extraction=True)\n",
    "encoder = encoder.to(device)\n",
    "#projection_sizes = [encoder.embed_dim, 2*encoder.embed_dim, 2*encoder.embed_dim, 2*encoder.embed_dim, 2*encoder.embed_dim, 2*encoder.embed_dim, \n",
    "#4*encoder.embed_dim, 4*encoder.embed_dim, 2*encoder.embed_dim]\n",
    "projection_sizes = [encoder.embed_dim,4*encoder.embed_dim, 4*encoder.embed_dim,4*encoder.embed_dim, 2*encoder.embed_dim]\n",
    "decoder = define_GPTCaption_model(encoder, projection_sizes=projection_sizes, use_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder = decoder.to(device)\n",
    "optimizer = optim.AdamW(decoder.parameters(), lr=LEARNING_RATE, weight_decay=0.2)\n",
    "#scheduler = get_linear_schedule_with_warmup(\n",
    "#    optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=NUM_EPOCHS*len(train_dl)\n",
    "#    )\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [1], gamma=0.5, verbose=True)\n",
    "use_amp = False\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "decoder.train()\n",
    "encoder.eval()\n",
    "decoder.to(device)\n",
    "print(\"\\n\\n\")\n",
    "#running_loss = []\n",
    "#running_semantic_accuracy = []\n",
    "#val_accuracy = []\n",
    "#lr_monitor = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"** Starting epoch {epoch} **\")\n",
    "    decoder.train()\n",
    "    with tqdm(train_dl, unit='batch') as tepoch:\n",
    "      semantic_accuracy = 0\n",
    "      for batch_idx, batch in enumerate(tepoch):\n",
    "          decoder.train()\n",
    "          tepoch.set_description(f\"Epoch: {epoch}\")\n",
    "\n",
    "          #batch_fmri = batch['fmri'].to(device)\n",
    "          batch_fmri = batch['fmri']\n",
    "          batch_fmri = batch_fmri.to(device)\n",
    "          batch_caption = batch['caption']\n",
    "\n",
    "          with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            #print(f\">>>> encoding fmri scans \", end=\"\")\n",
    "            fmri_prefix = encoder.forward(batch_fmri)\n",
    "            #print(f\"-> tokenizing captions \", end=\"\")\n",
    "            tokens, attention_mask = decoder.tokenizer(batch_caption, return_tensors=\"pt\", padding=True).values()\n",
    "            tokens, attention_mask, fmri_prefix = tokens.to(device), attention_mask.to(device), fmri_prefix.to(device)\n",
    "            #print(f\"-> decoding \")\n",
    "            outputs = decoder.forward(tokens, fmri_prefix, attention_mask)\n",
    "            logits = outputs.logits[:, decoder.prefix_length-1:-1]\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=decoder.tokenizer.pad_token_id)\n",
    "\n",
    "          decoder.zero_grad(set_to_none=True)\n",
    "          optimizer.zero_grad(set_to_none=True)\n",
    "          scaler.scale(loss).backward()\n",
    "          scaler.step(optimizer)\n",
    "          scaler.update()\n",
    "          \n",
    "          #print(f\">>>> batch {batch_idx} finished\", end=\"\\r\")\n",
    "\n",
    "          # Eval semantic accuracy\n",
    "          if batch_idx % 10 == 0:\n",
    "            decoder.eval()\n",
    "            with torch.no_grad():\n",
    "              generated_caption = decoder.generate_caption(fmri_prefix, device)\n",
    "              semantic_accuracy = torch.mean(calculate_semantic_similarity(generated_caption, batch_caption, device)).item()\n",
    "              running_semantic_accuracy.append(semantic_accuracy)\n",
    "              running_loss.append(loss.item())\n",
    "              lr_monitor.append(scheduler.get_last_lr())\n",
    "          \n",
    "          tepoch.set_postfix(loss=loss.item(), train_accuracy=semantic_accuracy)\n",
    "\n",
    "          # Free GPU memory\n",
    "          del batch_fmri, batch_caption, tokens, attention_mask, logits, outputs, loss\n",
    "          torch.cuda.empty_cache()\n",
    "    decoder.eval()\n",
    "    val_accuracy.append(calculate_accuracy_on_test(encoder, decoder, val_dl, device, return_best_batch=False))\n",
    "    print(f\"---- epoch {epoch} loss: {np.mean(running_loss[int(np.floor((epoch*len(train_dl))/10 + 1)):-1]):.4}, train accuracy: {np.mean(running_semantic_accuracy[int(np.floor((epoch*len(train_dl))/10 + 1)):-1]):.4}, validation accuracy (running, % above thresh): {val_accuracy[-1]} ---- \")\n",
    "    #scheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "acc, above_threshold = calculate_accuracy_on_test(encoder, decoder, test_dl, device, return_best_batch=False)\n",
    "print(acc, above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {\n",
    "    'comment': \"Small architecture on the CSI1_no_duplicates set. Transfer learning from no_dup_encoder_0.1555_0.01389_0.1748_22012023_09-45-51.pth. 16+16+16 epochs with lr=1.25e-5 Dropout=0.3. with decoder.eval(). started overfitting again. Last checkpoint ended with higher accuracy on test.\",\n",
    "    'hyperparameters': {'batch_size': BATCH_SIZE},\n",
    "    'decoder_projection': {'sizes': decoder.projection_sizes, 'sd': decoder.embedding_space_projection.state_dict()},\n",
    "    'optimizer': {'type': type(optimizer), 'sd': optimizer.state_dict()['param_groups']},\n",
    "    'scheduler': {'type': type(scheduler), 'sd': scheduler.state_dict()},\n",
    "    'training_data': {\n",
    "        \"running_loss\": running_loss,\n",
    "        'running_semantic_accuracy': running_semantic_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'lr_monitor': lr_monitor\n",
    "    }\n",
    "}\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y_%H-%M-%S\")\n",
    "torch.save(to_save, f'/databases/roeyshafran/BrainCap/Checkpoints/no_dup_encoder_{val_accuracy[-1][0]:.4}_{val_accuracy[-1][1]:.4}_{acc:.4}_{dt_string}.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = torch.load(r'/databases/roeyshafran/BrainCap/Checkpoints/no_dup_encoder_0.189_0.05556_0.18_22012023_12-20-13.pth')\n",
    "#model_dict = torch.load(r'/databases/roeyshafran/BrainCap/Checkpoints/encoder_0.1626_0.02778_20012023_11-54-46.pth')\n",
    "#loaded_decoder.load_state_dict(model_dict['decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_dict.keys())\n",
    "print(model_dict['comment'])\n",
    "#decoder.embedding_space_projection.load_state_dict(model_dict['decoder_projection']['sd'])\n",
    "running_loss = model_dict['training_data']['running_loss']\n",
    "running_semantic_accuracy = model_dict['training_data']['running_semantic_accuracy']\n",
    "val_accuracy = model_dict['training_data']['val_accuracy']\n",
    "lr_monitor = model_dict['training_data']['lr_monitor']\n",
    "#scheduler.load_state_dict(model_dict['scheduler']['sd'])\n",
    "#optimizer.load_state_dict(model_dict['optimizer'])\n",
    "new_sd = state_dict_MLP_to_MLP_dropout(decoder.embedding_space_projection.state_dict(), model_dict['decoder_projection']['sd'])\n",
    "print(decoder.embedding_space_projection.load_state_dict(new_sd))\n",
    "set_parameter_requires_grad(decoder.embedding_space_projection, feature_extraction=False)\n",
    "del model_dict\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder.embedding_space_projection.state_dict().keys())\n",
    "print(model_dict['decoder_projection']['sd'].keys())\n",
    "\n",
    "def state_dict_MLP_to_MLP_dropout(projection_state_dict, MLP_state_dict):\n",
    "    new_keys = dict(zip(MLP_state_dict.keys(), projection_state_dict.keys()))\n",
    "    new_sd = dict((new_keys[key], value) for (key, value) in MLP_state_dict.items())\n",
    "\n",
    "    return new_sd\n",
    "    \n",
    "#new_sd = state_dict_MLP_to_MLP_dropout(decoder.embedding_space_projection.state_dict(), model_dict['decoder_projection']['sd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.embedding_space_projection.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1, figsize=(10,10))\n",
    "axs = axs.flatten()\n",
    "#batch_iterations = np.arange(0, len(train_dl)*NUM_EPOCHS+1, 10)\n",
    "batch_iterations = np.arange(0, len(running_loss)*10, 10)\n",
    "val_iterations = np.arange(0, len(val_accuracy)*len(train_dl), len(train_dl))\n",
    "axs[0].plot(batch_iterations, running_loss)\n",
    "axs[0].set_xlabel('Batch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(batch_iterations, running_semantic_accuracy, label='Train')\n",
    "#axs[1].plot(batch_iterations[0:-1:int(np.ceil(len(running_loss)/10))], val_accuracy, '.', label='Validation')\n",
    "axs[1].plot(val_iterations, val_accuracy, '.', label='Validation')\n",
    "axs[1].set_xlabel('Batch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(batch_iterations[:-1:10], np.array(lr_monitor).squeeze()[0:-1:10])\n",
    "axs[2].set_xlabel('Batch')\n",
    "axs[2].set_ylabel('lr')\n",
    "\n",
    "#axs[3].plot(batch_iterations[:-1], val_accuracy)\n",
    "#axs[3].set_xlabel('Batch')\n",
    "#axs[3].set_ylabel('lr')\n",
    "\n",
    "fig.tight_layout() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.to(device)\n",
    "decoder.eval()\n",
    "# acc_test, above_thresh_test = calculate_accuracy_on_test(encoder, decoder, test_dl, device, return_best_batch=False)\n",
    "# acc_val, above_thresh_val = calculate_accuracy_on_test(encoder, decoder, val_dl, device, return_best_batch=False)\n",
    "# acc_train, above_thresh_train = calculate_accuracy_on_test(encoder, decoder, train_dl, device, return_best_batch=False)\n",
    "\n",
    "print(f\"Train: {acc_train}, {above_thresh_train*100}%\")\n",
    "print(f\"Validaion: {acc_val}, {above_thresh_val*100}%\")\n",
    "print(f\"Test: {acc_test}, {above_thresh_test*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Train: {acc_train}, {above_thresh_train*100}%\")\n",
    "# print(f\"Validaion: {acc_train}, {bove_thresh_val*100}%\")\n",
    "print(f\"Test: {acc_test}, {above_thresh_test*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dl:\n",
    "    bt = batch\n",
    "    break\n",
    "for batch in val_dl:\n",
    "    bv = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k_train = get_k_best(encoder, decoder, train_dl, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#acc, best_batch = calculate_accuracy_on_test(encoder, decoder, test_dl, device, return_best_batch=True)\n",
    "best_k_val = get_k_best_torch(encoder, decoder, val_dl, 2, device)\n",
    "best_k_train = get_k_best_torch(encoder, decoder, train_dl, 2, device)\n",
    "best_k_test = get_k_best_torch(encoder, decoder, test_dl, 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_k_train['image'] = best_k_train['image'].cpu()\n",
    "# best_k_train['fmri'] = best_k_train['fmri'].cpu()\n",
    "# best_k_train['accuracy'] = best_k_train['accuracy'].cpu()\n",
    "# best_k_train_records = [dict(zip(best_k_train,t)) for t in zip(*best_k_train.values())]\n",
    "\n",
    "best_k_test['image'] = best_k_test['image'].cpu()\n",
    "best_k_test['fmri'] = best_k_test['fmri'].cpu()\n",
    "best_k_test['accuracy'] = best_k_test['accuracy'].cpu()\n",
    "best_k_test_records = [dict(zip(best_k_test,t)) for t in zip(*best_k_test.values())]\n",
    "\n",
    "# best_k_val['image'] = best_k_val['image'].cpu()\n",
    "# best_k_val['fmri'] = best_k_val['fmri'].cpu()\n",
    "# best_k_val['accuracy'] = best_k_val['accuracy'].cpu()\n",
    "# best_k_val_records = [dict(zip(best_k_val,t)) for t in zip(*best_k_val.values())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_batch(best_k_test_records, fontsize=10, num_of_columns=5, caption_as_title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_batch(best_k_train_records, fontsize=10, num_of_columns=5, caption_as_title=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_batch(best_k_val_records, fontsize=10, num_of_columns=5, caption_as_title=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0326c91e7443510f411d303780687bd29b9f3e549e81f81e5049fd43ccfb4d79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
