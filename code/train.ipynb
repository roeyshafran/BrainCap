{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home1/roeyshafran/BrainCap/Mind-Cap/code/BrainCap.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7669736c646f636b2d636f6e64612d726f65797368616672616e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3133322e36382e35382e323437227d7d/home1/roeyshafran/BrainCap/Mind-Cap/code/BrainCap.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7669736c646f636b2d636f6e64612d726f65797368616672616e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3133322e36382e35382e323437227d7d/home1/roeyshafran/BrainCap/Mind-Cap/code/BrainCap.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home1/roeyshafran/BrainCap/Mind-Cap/code/Mind_Vis_utils/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7669736c646f636b2d636f6e64612d726f65797368616672616e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3133322e36382e35382e323437227d7d/home1/roeyshafran/BrainCap/Mind-Cap/code/BrainCap.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfmri_caption\u001b[39;00m \u001b[39mimport\u001b[39;00m GPTCaptionModel, create_fmri_encoder_from_pretrained,top_k_top_p_filtering, set_parameter_requires_grad, define_GPTCaption_model\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7669736c646f636b2d636f6e64612d726f65797368616672616e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3133322e36382e35382e323437227d7d/home1/roeyshafran/BrainCap/Mind-Cap/code/BrainCap.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m calculate_accuracy_on_test, calculate_semantic_similarity, state_dict_MLP_to_MLP_dropout, get_k_best_torch, print_batch\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7669736c646f636b2d636f6e64612d726f65797368616672616e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3133322e36382e35382e323437227d7d/home1/roeyshafran/BrainCap/Mind-Cap/code/BrainCap.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m BOLD5000_dataset, identity\n",
      "File \u001b[0;32m/home1/roeyshafran/BrainCap/Mind-Cap/code/fmri_caption.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m create_BOLD5000_dataset\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mMind_Vis_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdc_ldm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mldm_for_fmri\u001b[39;00m \u001b[39mimport\u001b[39;00m create_model_from_config \u001b[39mas\u001b[39;00m mindvis_create_model_from_config\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'/home1/roeyshafran/BrainCap/Mind-Cap/code/Mind_Vis_utils/')\n",
    "\n",
    "from fmri_caption import GPTCaptionModel, create_fmri_encoder_from_pretrained,top_k_top_p_filtering, set_parameter_requires_grad, define_GPTCaption_model\n",
    "from utils import calculate_accuracy_on_test, calculate_semantic_similarity, state_dict_MLP_to_MLP_dropout, get_k_best_torch, print_batch\n",
    "from dataset import BOLD5000_dataset, identity\n",
    "from dataset import create_BOLD5000_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "#import optuna\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Training on subject's ['CSI1']\n",
      "MinD-Vis pretrained encoder: /databases/roeyshafran/BrainCap/pretrains/pretrain_metafile.pth\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_from_checkpoint = False\n",
    "subjects_list = ['CSI1'] # ['CSI1', 'CSI2', 'CSI3', 'CSI4'], Only needed whrn create_dataset=True\n",
    "path_pretrained_fmri_encoder = r\"/databases/roeyshafran/BrainCap/pretrains/pretrain_metafile.pth\"\n",
    "path_presaved_dataset = r\"/databases/roeyshafran/BrainCap/data/CSI1_no_duplicates.pth\"\n",
    "path_checkpoints = '../data/Checkpoints'\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Training on subject's {subjects_list}\")\n",
    "print(f\"MinD-Vis pretrained encoder: {path_pretrained_fmri_encoder}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create BOLD5000 dataset\n",
    "BOLD_dataset = torch.load(path_presaved_dataset)\n",
    "bold_train, bold_test = BOLD_dataset['train'], BOLD_dataset['test']\n",
    "#num_voxels = bold_test.num_voxels\n",
    "num_voxels = 1696\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(range(len(bold_train))),test_size=0.1)\n",
    "bold_val = Subset(bold_train, val_idx)\n",
    "bold_train = Subset(bold_train, train_idx)\n",
    "\n",
    "train_dl = DataLoader(bold_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(bold_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(bold_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Train length: {len(bold_train)}, Validation length: {len(bold_val)}, Test length: {len(bold_test)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Model Initialization and Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1.25e-5\n",
    "NUM_EPOCHS = 16\n",
    "weight_decay = 0.2\n",
    "use_amp = False\n",
    "train_from_checkpoint = False\n",
    "scheduler_milestones = []\n",
    "scheduler_gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 262 to 106\n",
      "missing keys: ['mask_token']\n",
      "unexpected keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n"
     ]
    }
   ],
   "source": [
    "# Get encoder-decoder\n",
    "encoder = create_fmri_encoder_from_pretrained(path_pretrained_fmri_encoder, num_voxels, feature_extraction=True)\n",
    "encoder = encoder.to(device)\n",
    "projection_sizes = [encoder.embed_dim,4*encoder.embed_dim, 4*encoder.embed_dim,4*encoder.embed_dim, 2*encoder.embed_dim]\n",
    "decoder = define_GPTCaption_model(encoder, projection_sizes=projection_sizes, use_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If train_from_checkpoint == True, load checkpoint.\n",
    "checkpoint_name = r'middle_checkpoint_dropout_starts_from_hereno_dup_encoder_0.189_0.05556_0.18_22012023_12-20-13.pth'\n",
    "if train_from_checkpoint:\n",
    "    model_dict = torch.load(os.path.join(path_checkpoints, checkpoint_name))\n",
    "    print(f\"Loaded checkpoin comment: {model_dict['comment']}\")\n",
    "    \n",
    "    # Loading training data to visualize the complete training process\n",
    "    running_loss = model_dict['training_data']['running_loss']\n",
    "    running_semantic_accuracy = model_dict['training_data']['running_semantic_accuracy']\n",
    "    val_accuracy = model_dict['training_data']['val_accuracy']\n",
    "    lr_monitor = model_dict['training_data']['lr_monitor']\n",
    "    \n",
    "    new_sd = state_dict_MLP_to_MLP_dropout(decoder.embedding_space_projection.state_dict(), model_dict['decoder_projection']['sd'])\n",
    "    print(decoder.embedding_space_projection.load_state_dict(new_sd))\n",
    "    set_parameter_requires_grad(decoder.embedding_space_projection, feature_extraction=False)\n",
    "    del model_dict\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2500e-05.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(decoder.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, scheduler_milestones, gamma=scheduler_gamma, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "** Starting epoch 0 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 243/243 [01:25<00:00,  2.85batch/s, loss=4.16, train_accuracy=0.0963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- epoch 0 loss: 3.616, train accuracy: 0.07682, validation accuracy (running, % above thresh): (0.08603957254025671, 0.0) ---- \n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "** Starting epoch 1 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 243/243 [01:18<00:00,  3.11batch/s, loss=3.71, train_accuracy=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- epoch 1 loss: 3.691, train accuracy: 0.08094, validation accuracy (running, % above thresh): (0.10064912063104135, 0.004629629629629629) ---- \n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "** Starting epoch 2 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 243/243 [01:21<00:00,  2.99batch/s, loss=3.18, train_accuracy=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- epoch 2 loss: 3.461, train accuracy: 0.09042, validation accuracy (running, % above thresh): (0.09252708350066785, 0.004629629629629629) ---- \n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "** Starting epoch 3 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 243/243 [01:14<00:00,  3.26batch/s, loss=3.11, train_accuracy=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- epoch 3 loss: 3.648, train accuracy: 0.0947, validation accuracy (running, % above thresh): (0.10687647911685484, 0.004629629629629629) ---- \n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "** Starting epoch 4 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 243/243 [01:18<00:00,  3.08batch/s, loss=3.61, train_accuracy=0.083]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- epoch 4 loss: 3.618, train accuracy: 0.09917, validation accuracy (running, % above thresh): (0.09287556660947976, 0.0) ---- \n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "** Starting epoch 5 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 243/243 [01:14<00:00,  3.26batch/s, loss=3.04, train_accuracy=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- epoch 5 loss: 3.563, train accuracy: 0.09804, validation accuracy (running, % above thresh): (0.09744298030380849, 0.0) ---- \n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "** Starting epoch 6 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 243/243 [01:18<00:00,  3.11batch/s, loss=4.16, train_accuracy=0.147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- epoch 6 loss: 3.473, train accuracy: 0.1112, validation accuracy (running, % above thresh): (0.09927248561547862, 0.0) ---- \n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "** Starting epoch 7 **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 243/243 [01:17<00:00,  3.14batch/s, loss=3.07, train_accuracy=0.0161]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m       torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m     58\u001b[0m decoder\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 59\u001b[0m val_accuracy\u001b[39m.\u001b[39mappend(calculate_accuracy_on_test(encoder, decoder, val_dl, device, return_best_batch\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m---- epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(running_loss[\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mfloor((epoch\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(train_dl))\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)):\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m:\u001b[39;00m\u001b[39m.4\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, train accuracy: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(running_semantic_accuracy[\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mfloor((epoch\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(train_dl))\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)):\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m:\u001b[39;00m\u001b[39m.4\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, validation accuracy (running, % above thresh): \u001b[39m\u001b[39m{\u001b[39;00mval_accuracy[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m ---- \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home1/roeyshafran/BrainCap/Mind-Cap/code/utils.py:90\u001b[0m, in \u001b[0;36mcalculate_accuracy_on_test\u001b[0;34m(encoder, decoder, dataloader, device, threshhold, return_best_batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m fmri_prefix \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mforward(batch[\u001b[39m'\u001b[39m\u001b[39mfmri\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     88\u001b[0m generated_caption \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39mgenerate_caption(fmri_prefix, device)\n\u001b[0;32m---> 90\u001b[0m accuracy_tensor \u001b[39m=\u001b[39m calculate_semantic_similarity(generated_caption, batch[\u001b[39m'\u001b[39;49m\u001b[39mcaption\u001b[39;49m\u001b[39m'\u001b[39;49m], device)\n\u001b[1;32m     91\u001b[0m above_threshhold_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnumel(accuracy_tensor[accuracy_tensor \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshhold])\n\u001b[1;32m     92\u001b[0m accuracy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(accuracy_tensor)\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home1/roeyshafran/BrainCap/Mind-Cap/code/utils.py:26\u001b[0m, in \u001b[0;36mcalculate_semantic_similarity\u001b[0;34m(generated_caption, real_caption, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_semantic_similarity\u001b[39m(generated_caption, real_caption, device):\n\u001b[0;32m---> 26\u001b[0m     sentence_model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39;49m\u001b[39mall-mpnet-base-v2\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m     embed_generated \u001b[39m=\u001b[39m sentence_model\u001b[39m.\u001b[39mencode(generated_caption, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m     embed_real_caption \u001b[39m=\u001b[39m sentence_model\u001b[39m.\u001b[39mencode(real_caption, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[1;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[1;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(sbert_config_path) \u001b[39mas\u001b[39;00m fIn:\n\u001b[1;32m    136\u001b[0m     config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m Transformer(model_name_or_path\u001b[39m=\u001b[39;49minput_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m     28\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[0;32m---> 29\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[39mif\u001b[39;00m tokenizer_name_or_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_name_or_path, cache_dir\u001b[39m=\u001b[39mcache_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_args)\n\u001b[1;32m     33\u001b[0m \u001b[39m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path, config\u001b[39m=\u001b[39;49mconfig, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    465\u001b[0m     )\n\u001b[1;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/transformers/modeling_utils.py:2276\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2273\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2275\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2276\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2278\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit:\n\u001b[1;32m   2279\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbitsandbytes\u001b[39;00m \u001b[39mimport\u001b[39;00m get_keys_to_not_convert, replace_8bit_linear\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/transformers/models/mpnet/modeling_mpnet.py:491\u001b[0m, in \u001b[0;36mMPNetModel.__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 491\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m MPNetEmbeddings(config)\n\u001b[1;32m    492\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m MPNetEncoder(config)\n\u001b[1;32m    493\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39m=\u001b[39m MPNetPooler(config) \u001b[39mif\u001b[39;00m add_pooling_layer \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/transformers/models/mpnet/modeling_mpnet.py:80\u001b[0m, in \u001b[0;36mMPNetEmbeddings.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(config\u001b[39m.\u001b[39;49mvocab_size, config\u001b[39m.\u001b[39;49mhidden_size, padding_idx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx)\n\u001b[1;32m     81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(\n\u001b[1;32m     82\u001b[0m     config\u001b[39m.\u001b[39mmax_position_embeddings, config\u001b[39m.\u001b[39mhidden_size, padding_idx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_eps)\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/torch/nn/modules/sparse.py:142\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlist\u001b[39m(_weight\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[1;32m    145\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/torch/nn/modules/sparse.py:151\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     init\u001b[39m.\u001b[39;49mnormal_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[0;32m/opt/conda/envs/brain-cap/lib/python3.9/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decoder.train()\n",
    "encoder.eval()\n",
    "decoder.to(device)\n",
    "\n",
    "if not train_from_checkpoint:\n",
    "  running_loss = []\n",
    "  running_semantic_accuracy = []\n",
    "  val_accuracy = []\n",
    "  lr_monitor = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n---- Starting epoch {epoch} ----\")\n",
    "    decoder.train()\n",
    "    with tqdm(train_dl, unit='batch') as tepoch:\n",
    "      semantic_accuracy = 0\n",
    "      for batch_idx, batch in enumerate(tepoch):\n",
    "          decoder.train()\n",
    "          tepoch.set_description(f\"Epoch: {epoch}\")\n",
    "\n",
    "          #batch_fmri = batch['fmri'].to(device)\n",
    "          batch_fmri = batch['fmri']\n",
    "          batch_fmri = batch_fmri.to(device)\n",
    "          batch_caption = batch['caption']\n",
    "\n",
    "          with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            fmri_prefix = encoder.forward(batch_fmri)\n",
    "            tokens, attention_mask = decoder.tokenizer(batch_caption, return_tensors=\"pt\", padding=True).values()\n",
    "            tokens, attention_mask, fmri_prefix = tokens.to(device), attention_mask.to(device), fmri_prefix.to(device)\n",
    "            outputs = decoder.forward(tokens, fmri_prefix, attention_mask)\n",
    "            logits = outputs.logits[:, decoder.prefix_length-1:-1]\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=decoder.tokenizer.pad_token_id)\n",
    "\n",
    "          decoder.zero_grad(set_to_none=True)\n",
    "          optimizer.zero_grad(set_to_none=True)\n",
    "          scaler.scale(loss).backward()\n",
    "          scaler.step(optimizer)\n",
    "          scaler.update()\n",
    "\n",
    "          # Evaluate model\n",
    "          if batch_idx % 10 == 0:\n",
    "            decoder.eval()\n",
    "            with torch.no_grad():\n",
    "              generated_caption = decoder.generate_caption(fmri_prefix, device)\n",
    "              semantic_accuracy = torch.mean(calculate_semantic_similarity(generated_caption, batch_caption, device)).item()\n",
    "              running_semantic_accuracy.append(semantic_accuracy)\n",
    "              running_loss.append(loss.item())\n",
    "              lr_monitor.append(scheduler.get_last_lr())\n",
    "          \n",
    "          tepoch.set_postfix(loss=loss.item(), train_accuracy=semantic_accuracy)\n",
    "\n",
    "          # Free GPU memory, this was needed to prevent our GPU RAM from reaching capacity\n",
    "          del batch_fmri, batch_caption, tokens, attention_mask, logits, outputs, loss\n",
    "          torch.cuda.empty_cache()\n",
    "    decoder.eval()\n",
    "    val_accuracy.append(calculate_accuracy_on_test(encoder, decoder, val_dl, device, return_best_batch=False))\n",
    "    print(f\"---- epoch {epoch} loss: {np.mean(running_loss[int(np.floor((epoch*len(train_dl))/10 + 1)):-1]):.4}, train accuracy: {np.mean(running_semantic_accuracy[int(np.floor((epoch*len(train_dl))/10 + 1)):-1]):.4}, validation accuracy (running, % above thresh): {val_accuracy[-1]} ---- \")\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuazlie Training\n",
    "\n",
    "fig, axs = plt.subplots(3,1, figsize=(10,10))\n",
    "axs = axs.flatten()\n",
    "#batch_iterations = np.arange(0, len(train_dl)*NUM_EPOCHS+1, 10)\n",
    "batch_iterations = np.arange(0, len(running_loss)*10, 10)\n",
    "val_iterations = np.arange(0, len(val_accuracy)*len(train_dl), len(train_dl))\n",
    "axs[0].plot(batch_iterations, running_loss)\n",
    "axs[0].set_xlabel('Batch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(batch_iterations, running_semantic_accuracy, label='Train')\n",
    "axs[1].plot(val_iterations, val_accuracy, '.', label=('Validation', 'Validation - above 0.5'))\n",
    "axs[1].set_xlabel('Batch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(batch_iterations[:-1:10], np.array(lr_monitor).squeeze()[0:-1:10])\n",
    "axs[2].set_xlabel('Batch')\n",
    "axs[2].set_ylabel('lr')\n",
    "\n",
    "fig.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies\n",
    "decoder.to(device)\n",
    "decoder.eval()\n",
    "accuracy_test, above_thresh_test = calculate_accuracy_on_test(encoder, decoder, test_dl, device, return_best_batch=False)\n",
    "accuracy_val, above_thresh_val = calculate_accuracy_on_test(encoder, decoder, val_dl, device, return_best_batch=False)\n",
    "accuracy_train, above_thresh_train = calculate_accuracy_on_test(encoder, decoder, train_dl, device, return_best_batch=False)\n",
    "\n",
    "print(f\"Train: {accuracy_train}, {above_thresh_train*100}%\")\n",
    "print(f\"Validaion: {accuracy_val}, {above_thresh_val*100}%\")\n",
    "print(f\"Test: {accuracy_test}, {above_thresh_test*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Best Samples\n",
    "K = 2\n",
    "best_k_val = get_k_best_torch(encoder, decoder, val_dl, K, device)\n",
    "best_k_train = get_k_best_torch(encoder, decoder, train_dl, K, device)\n",
    "best_k_test = get_k_best_torch(encoder, decoder, test_dl, K, device)\n",
    "\n",
    "def best_k_dict_to_records(best_k):\n",
    "    best_k['image'] = best_k['image'].cpu()\n",
    "    best_k['fmri'] = best_k['fmri'].cpu()\n",
    "    best_k['accuracy'] = best_k['accuracy'].cpu()\n",
    "    best_k_records = [dict(zip(best_k,t)) for t in zip(*best_k.values())]\n",
    "\n",
    "    return best_k_records\n",
    "\n",
    "best_k_train_records = best_k_dict_to_records(best_k_train)\n",
    "best_k_test_records = best_k_dict_to_records(best_k_test)\n",
    "best_k_val_records = best_k_dict_to_records(best_k_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best results\n",
    "# print_batch(best_k_train_records, fontsize=10, num_of_columns=2, caption_as_title=False)\n",
    "# print_batch(best_k_val_records, fontsize=10, num_of_columns=2, caption_as_title=False)\n",
    "print_batch(best_k_test_records, fontsize=10, num_of_columns=2, caption_as_title=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy is used for the file name. Run calculate accuracies cell before.\n",
    "to_save = {\n",
    "    'comment': \"You can add some comments on the checkpoint\",\n",
    "    'hyperparameters': {'batch_size': BATCH_SIZE},\n",
    "    'decoder_projection': {'sizes': decoder.projection_sizes, 'sd': decoder.embedding_space_projection.state_dict()},\n",
    "    'optimizer': {'type': type(optimizer), 'optimizer_param_groups': optimizer.state_dict()['param_groups']},\n",
    "    'scheduler': {'type': type(scheduler), 'sd': scheduler.state_dict()},\n",
    "    'training_data': {\n",
    "        \"running_loss\": running_loss,\n",
    "        'running_semantic_accuracy': running_semantic_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'lr_monitor': lr_monitor\n",
    "    }\n",
    "}\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y_%H-%M-%S\")\n",
    "torch.save(to_save, os.path.join(path_checkpoints, f\"decoder_test_accuracy_{accuracy_test:.4}_{dt_string}.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "0326c91e7443510f411d303780687bd29b9f3e549e81f81e5049fd43ccfb4d79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
