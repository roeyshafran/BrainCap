{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'./Mind_Vis_utils/')\n",
    "\n",
    "from fmri_caption import GPTCaptionModel, create_fmri_encoder_from_pretrained,top_k_top_p_filtering, set_parameter_requires_grad, define_GPTCaption_model\n",
    "from utils import calculate_accuracy_on_test, calculate_semantic_similarity, state_dict_MLP_to_MLP_dropout, get_k_best_torch, print_batch\n",
    "from dataset import BOLD5000_dataset, identity\n",
    "from dataset import create_BOLD5000_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "#import optuna\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Training on subject's ['CSI1']\n",
      "MinD-Vis pretrained encoder: /databases/roeyshafran/BrainCap/pretrains/pretrain_metafile.pth\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_from_checkpoint = False\n",
    "subjects_list = ['CSI1'] # ['CSI1', 'CSI2', 'CSI3', 'CSI4'], Only needed when create_dataset=True\n",
    "path_pretrained_fmri_encoder = r\"../pretrains/fmri_encoder_pretrain_metafile.pth\"\n",
    "path_presaved_dataset = r\"../data/BOLD5000/CSI1_no_duplicates.pth\"\n",
    "path_checkpoints = '../data/Checkpoints'\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Training on subject's {subjects_list}\")\n",
    "print(f\"MinD-Vis pretrained encoder: {path_pretrained_fmri_encoder}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create BOLD5000 dataset\n",
    "BOLD_dataset = torch.load(path_presaved_dataset)\n",
    "bold_train, bold_test = BOLD_dataset['train'], BOLD_dataset['test']\n",
    "num_voxels = bold_test.num_voxels\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(range(len(bold_train))),test_size=0.1)\n",
    "bold_val = Subset(bold_train, val_idx)\n",
    "bold_train = Subset(bold_train, train_idx)\n",
    "\n",
    "print(f\"Train length: {len(bold_train)}, Validation length: {len(bold_val)}, Test length: {len(bold_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Model Initialization and Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1.25e-5\n",
    "NUM_EPOCHS = 16\n",
    "weight_decay = 0.2\n",
    "use_amp = False\n",
    "train_from_checkpoint = True # Set to True if you want to load previous checkout and train from there.\n",
    "scheduler_milestones = []\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "train_dl = DataLoader(bold_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(bold_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(bold_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 262 to 106\n",
      "missing keys: ['mask_token']\n",
      "unexpected keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n"
     ]
    }
   ],
   "source": [
    "# Get encoder-decoder\n",
    "encoder = create_fmri_encoder_from_pretrained(path_pretrained_fmri_encoder, num_voxels, feature_extraction=True)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# The MLP mapping network architecture\n",
    "projection_sizes = [encoder.embed_dim,4*encoder.embed_dim, 4*encoder.embed_dim,4*encoder.embed_dim, 2*encoder.embed_dim]\n",
    "\n",
    "# Make sure to set use_dropout if you are loading a checkout that was trained with dropout (final checkpoint for example)\n",
    "decoder = define_GPTCaption_model(encoder, projection_sizes=projection_sizes, use_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If train_from_checkpoint == True, load checkpoint.\n",
    "checkpoint_name = r'decoder_22012023_12-20-13.pth'\n",
    "if train_from_checkpoint:\n",
    "    model_dict = torch.load(os.path.join(path_checkpoints, checkpoint_name))\n",
    "    print(f\"Loaded checkpoin comment: {model_dict['comment']}\")\n",
    "    \n",
    "    # Loading training data to visualize the complete training process\n",
    "    running_loss = model_dict['training_data']['running_loss']\n",
    "    running_semantic_accuracy = model_dict['training_data']['running_semantic_accuracy']\n",
    "    val_accuracy = model_dict['training_data']['val_accuracy']\n",
    "    lr_monitor = model_dict['training_data']['lr_monitor']\n",
    "    \n",
    "    new_sd = state_dict_MLP_to_MLP_dropout(decoder.embedding_space_projection.state_dict(), model_dict['decoder_projection']['sd'])\n",
    "    print(decoder.embedding_space_projection.load_state_dict(new_sd))\n",
    "    set_parameter_requires_grad(decoder.embedding_space_projection, feature_extraction=False)\n",
    "    del model_dict\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2500e-05.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(decoder.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, scheduler_milestones, gamma=scheduler_gamma, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder.train()\n",
    "encoder.eval()\n",
    "decoder.to(device)\n",
    "\n",
    "# Use saved training data if training from a checkpoint\n",
    "if not train_from_checkpoint:\n",
    "  running_loss = []\n",
    "  running_semantic_accuracy = []\n",
    "  val_accuracy = []\n",
    "  lr_monitor = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n---- Starting epoch {epoch} ----\")\n",
    "    decoder.train()\n",
    "    with tqdm(train_dl, unit='batch') as tepoch:\n",
    "      semantic_accuracy = 0\n",
    "      for batch_idx, batch in enumerate(tepoch):\n",
    "          decoder.train()\n",
    "          tepoch.set_description(f\"Epoch: {epoch}\")\n",
    "\n",
    "          #batch_fmri = batch['fmri'].to(device)\n",
    "          batch_fmri = batch['fmri']\n",
    "          batch_fmri = batch_fmri.to(device)\n",
    "          batch_caption = batch['caption']\n",
    "\n",
    "          with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            fmri_prefix = encoder.forward(batch_fmri)\n",
    "            tokens, attention_mask = decoder.tokenizer(batch_caption, return_tensors=\"pt\", padding=True).values()\n",
    "            tokens, attention_mask, fmri_prefix = tokens.to(device), attention_mask.to(device), fmri_prefix.to(device)\n",
    "            outputs = decoder.forward(tokens, fmri_prefix, attention_mask)\n",
    "            logits = outputs.logits[:, decoder.prefix_length-1:-1]\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=decoder.tokenizer.pad_token_id)\n",
    "\n",
    "          decoder.zero_grad(set_to_none=True)\n",
    "          optimizer.zero_grad(set_to_none=True)\n",
    "          scaler.scale(loss).backward()\n",
    "          scaler.step(optimizer)\n",
    "          scaler.update()\n",
    "\n",
    "          # Evaluate model\n",
    "          if batch_idx % 10 == 0:\n",
    "            decoder.eval()\n",
    "            with torch.no_grad():\n",
    "              generated_caption = decoder.generate_caption(fmri_prefix, device)\n",
    "              semantic_accuracy = torch.mean(calculate_semantic_similarity(generated_caption, batch_caption, device)).item()\n",
    "              running_semantic_accuracy.append(semantic_accuracy)\n",
    "              running_loss.append(loss.item())\n",
    "              lr_monitor.append(scheduler.get_last_lr())\n",
    "          \n",
    "          tepoch.set_postfix(loss=loss.item(), train_accuracy=semantic_accuracy)\n",
    "\n",
    "          # Free GPU memory, this was needed to prevent our GPU RAM from reaching capacity\n",
    "          del batch_fmri, batch_caption, tokens, attention_mask, logits, outputs, loss\n",
    "          torch.cuda.empty_cache()\n",
    "    decoder.eval()\n",
    "    val_accuracy.append(calculate_accuracy_on_test(encoder, decoder, val_dl, device, return_best_batch=False))\n",
    "    print(f\"---- epoch {epoch} loss: {np.mean(running_loss[int(np.floor((epoch*len(train_dl))/10 + 1)):-1]):.4}, train accuracy: {np.mean(running_semantic_accuracy[int(np.floor((epoch*len(train_dl))/10 + 1)):-1]):.4}, validation accuracy (running, % above thresh): {val_accuracy[-1]} ---- \")\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuazlie Training\n",
    "\n",
    "fig, axs = plt.subplots(3,1, figsize=(10,10))\n",
    "axs = axs.flatten()\n",
    "#batch_iterations = np.arange(0, len(train_dl)*NUM_EPOCHS+1, 10)\n",
    "batch_iterations = np.arange(0, len(running_loss)*10, 10)\n",
    "val_iterations = np.arange(0, len(val_accuracy)*len(train_dl), len(train_dl))\n",
    "axs[0].plot(batch_iterations, running_loss)\n",
    "axs[0].set_xlabel('Batch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(batch_iterations, running_semantic_accuracy, label='Train')\n",
    "axs[1].plot(val_iterations, val_accuracy, '.', label=('Validation', 'Validation - above 0.5'))\n",
    "axs[1].set_xlabel('Batch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(batch_iterations[:-1:10], np.array(lr_monitor).squeeze()[0:-1:10])\n",
    "axs[2].set_xlabel('Batch')\n",
    "axs[2].set_ylabel('lr')\n",
    "\n",
    "fig.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies\n",
    "decoder.to(device)\n",
    "decoder.eval()\n",
    "accuracy_test, above_thresh_test = calculate_accuracy_on_test(encoder, decoder, test_dl, device, return_best_batch=False)\n",
    "accuracy_val, above_thresh_val = calculate_accuracy_on_test(encoder, decoder, val_dl, device, return_best_batch=False)\n",
    "accuracy_train, above_thresh_train = calculate_accuracy_on_test(encoder, decoder, train_dl, device, return_best_batch=False)\n",
    "\n",
    "print(f\"Train: {accuracy_train}, {above_thresh_train*100}%\")\n",
    "print(f\"Validaion: {accuracy_val}, {above_thresh_val*100}%\")\n",
    "print(f\"Test: {accuracy_test}, {above_thresh_test*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Best Samples\n",
    "K = 2\n",
    "best_k_val = get_k_best_torch(encoder, decoder, val_dl, K, device)\n",
    "best_k_train = get_k_best_torch(encoder, decoder, train_dl, K, device)\n",
    "best_k_test = get_k_best_torch(encoder, decoder, test_dl, K, device)\n",
    "\n",
    "def best_k_dict_to_records(best_k):\n",
    "    best_k['image'] = best_k['image'].cpu()\n",
    "    best_k['fmri'] = best_k['fmri'].cpu()\n",
    "    best_k['accuracy'] = best_k['accuracy'].cpu()\n",
    "    best_k_records = [dict(zip(best_k,t)) for t in zip(*best_k.values())]\n",
    "\n",
    "    return best_k_records\n",
    "\n",
    "best_k_train_records = best_k_dict_to_records(best_k_train)\n",
    "best_k_test_records = best_k_dict_to_records(best_k_test)\n",
    "best_k_val_records = best_k_dict_to_records(best_k_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best results\n",
    "# print_batch(best_k_train_records, fontsize=10, num_of_columns=2, caption_as_title=False)\n",
    "# print_batch(best_k_val_records, fontsize=10, num_of_columns=2, caption_as_title=False)\n",
    "print_batch(best_k_test_records, fontsize=10, num_of_columns=2, caption_as_title=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy is used for the file name. Run calculate accuracies cell before.\n",
    "# Change saved parameters as needed\n",
    "to_save = {\n",
    "    'comment': \"You can add some comments on the checkpoint\",\n",
    "    'hyperparameters': {'batch_size': BATCH_SIZE},\n",
    "    'decoder_projection': {'sizes': decoder.projection_sizes, 'sd': decoder.embedding_space_projection.state_dict()},\n",
    "    'optimizer': {'type': type(optimizer), 'optimizer_param_groups': optimizer.state_dict()['param_groups']},\n",
    "    'scheduler': {'type': type(scheduler), 'sd': scheduler.state_dict()},\n",
    "    'training_data': {\n",
    "        \"running_loss\": running_loss,\n",
    "        'running_semantic_accuracy': running_semantic_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'lr_monitor': lr_monitor\n",
    "    }\n",
    "}\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y_%H-%M-%S\")\n",
    "torch.save(to_save, os.path.join(path_checkpoints, f\"decoder_test_accuracy_{accuracy_test:.4}_{dt_string}.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0326c91e7443510f411d303780687bd29b9f3e549e81f81e5049fd43ccfb4d79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
